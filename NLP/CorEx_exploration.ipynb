{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# !pip3 install corextopic\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic as vt\n",
    "import scipy.sparse as ss\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim_models\n",
    "# pyLDAvis.enable_notebook()\n",
    "\n",
    "# import gensim\n",
    "# from gensim.utils import simple_preprocess\n",
    "# from gensim.parsing.preprocessing import STOPWORDS\n",
    "# from gensim.corpora import Dictionary\n",
    "# def simple_tokenize(text):\n",
    "#   return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "# from nltk.stem.porter import *\n",
    "\n",
    "def flatten_list(l):\n",
    "  return [item for sublist in l for item in sublist]\n",
    "\n",
    "def describe_training_documents(list_of_docs):\n",
    "  print('There are',len(list_of_docs),'documents.')\n",
    "  document_lengths = list(map(lambda x: len(x.split()),list_of_docs))\n",
    "  percentile_50 = int(np.percentile(document_lengths,50))\n",
    "  percentile_95 = int(np.percentile(document_lengths,95))\n",
    "  print('95% of the documents are below:',percentile_95,'words.')\n",
    "  plt.axvline(percentile_50, lw=1, color='g')\n",
    "  plt.axvline(percentile_95, lw=1, color='r', linestyle='--')\n",
    "  _ = plt.hist(document_lengths, bins=50, range=(0,percentile_95+100))\n",
    "  print('Solid green line indicates median, dotted red line indicates 95 percentile. Outliers may be cropped.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchored Correlation Explanation:Topic Modeling with Minimal Domain Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 20 newsgroups data\n",
    "newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "documents_train = list(np.load('data/train.npy')) # historical materials 4451 documents\n",
    "documents_train2 = list(np.load('data/train2.npy'))   # census bureau 4226 documents\n",
    "df_occsc = pd.read_csv('data/OCC_pairs.csv').rename(columns={'OCC_DES':'Full Occupation'})\n",
    "assert(df_occsc['Full Occupation'].nunique() == len(df_occsc))\n",
    "occ_list = list(set(list(df_occsc['Full Occupation'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5878 documents.\n",
      "95% of the documents are below: 67 words.\n",
      "Solid green line indicates median, dotted red line indicates 95 percentile. Outliers may be cropped.\n"
     ]
    }
   ],
   "source": [
    "with open(\"nyt_text_modified.txt\",'r') as f:\n",
    "    nyt_text2 = f.readlines()\n",
    "describe_training_documents(nyt_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the dataset there are 14991 textual documents\n",
      "And this is the first one:\n",
      " Skip to main content Search UPLOAD SIGN UP | LOG IN BOOKS VIDEO AUDIO SOFTWARE IMAGESABOUT BLOG PROJECTS HELP DONATE  CONTACT JOBS VOLUNTEER PEOPLE Search Metadata Search text contents Search TV news captions Search radio transcripts Search archived websitesAdvanced SearchSign up for freeLog inFull text of \"The practical cabinet maker and furniture designer's assistant, with essays on history of furniture, taste in design, color and materials, with full explanation of the canons of good taste in furniture ..\"See other formats^ \n"
     ]
    }
   ],
   "source": [
    "# include both training and testing dataset into the vectorizer\n",
    "# but fit the model with the training dataset\n",
    "# corex model requires them to be the same shape \n",
    "documents = []\n",
    "documents.extend(documents_train)\n",
    "documents.extend(documents_train2)\n",
    "documents.extend(nyt_text2)\n",
    "\n",
    "document_total = documents[:]\n",
    "document_total.extend(occ_list)\n",
    "\n",
    "print(\"In the dataset there are\", len(document_total), \"textual documents\")\n",
    "print(\"And this is the first one:\\n\", documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic model assumes input is in the form of a doc-word matrix, where rows are documents and columns are binary counts. We'll vectorize the dataset, take the top 10,000 words, and convert it to a sparse matrix to save on memory usage. Note, we use binary count vectors as input to the CorEx topic model.\n",
    "\n",
    "### Transform data into a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14991, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_features=10000, binary=True)\n",
    "doc_word = vectorizer.fit_transform(document_total)\n",
    "doc_word = ss.csr_matrix(doc_word)\n",
    "\n",
    "doc_word.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14991, 9222)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))\n",
    "\n",
    "not_digit_inds = [ind for ind,word in enumerate(words) if not word.isdigit()]\n",
    "doc_word = doc_word[:,not_digit_inds]\n",
    "words = [word for ind,word in enumerate(words) if not word.isdigit()]\n",
    "\n",
    "doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9222"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corex, rep size: 50\n",
      "WARNING: Some words never appear (or always appear)\n",
      "word counts [5. 7. 4. ... 4. 4. 4.]\n",
      "[0.013 0.014 0.013 0.013 0.015 0.01  0.013 0.013 0.01  0.011 0.013 0.015 0.016 0.013 0.012 0.012 0.012 0.015 0.013 0.009 0.011 0.012 0.013 0.011 0.023 0.014 0.014 0.012 0.014 0.013 0.011 0.014 0.011\n",
      " 0.014 0.015 0.012 0.015 0.015 0.011 0.012 0.015 0.014 0.014 0.014 0.012 0.011 0.013 0.018 0.011 0.013]\n",
      "[0.281 0.427 0.288 0.266 0.483 0.265 0.264 0.399 0.255 0.259 0.314 0.436 0.408 0.297 0.352 0.309 0.274 0.392 0.248 0.241 0.458 0.307 0.471 0.284 0.878 0.314 0.38  0.316 0.353 0.412 0.289 0.437 0.288\n",
      " 0.31  0.422 0.293 0.345 0.385 0.253 0.255 0.423 0.373 0.307 0.308 0.287 0.245 0.377 0.561 0.288 0.31 ]\n",
      "[0.66  1.579 1.044 0.78  2.11  0.648 0.57  1.526 0.537 0.68  1.023 1.515 1.301 0.718 1.61  0.994 0.964 1.316 0.607 0.626 1.981 1.068 2.134 0.9   2.714 1.069 1.505 1.101 1.343 1.949 0.715 1.321 1.015\n",
      " 0.901 1.436 0.664 0.879 1.272 0.658 0.769 1.368 1.204 0.756 0.914 0.702 0.535 1.197 1.835 1.006 0.991]\n",
      "[0.208 0.964 0.305 0.318 0.544 0.31  0.216 0.191 0.243 0.355 0.375 0.821 0.206 0.195 0.585 0.299 0.302 0.614 0.176 0.227 1.037 0.256 0.563 0.248 0.146 0.728 0.444 0.445 0.435 2.27  0.201 0.237 0.242\n",
      " 0.288 0.084 0.195 0.31  0.155 0.307 0.209 1.152 0.545 0.18  0.205 0.246 0.224 0.498 0.415 0.362 0.321]\n",
      "[0.678 0.749 0.796 0.716 0.814 0.528 0.515 1.22  0.458 0.645 0.643 0.698 1.102 0.688 0.666 0.722 0.799 0.568 0.736 0.655 0.966 0.884 0.717 0.67  1.569 0.781 0.824 0.656 0.68  2.015 0.67  0.913 0.929\n",
      " 0.726 2.247 0.732 0.6   1.255 0.489 0.904 1.182 0.561 0.669 0.826 0.613 0.632 0.601 0.765 0.678 0.634]\n",
      "[ 0.008  0.517 -0.06   0.146  0.218  0.241  0.023  0.012  0.145  0.412  0.082  0.542  0.053 -0.005  0.43   0.054  0.135  0.37  -0.061 -0.002  0.669  0.049  0.261  0.05   0.003  0.605  0.312  0.629\n",
      "  0.17   1.691 -0.028  0.118 -0.076  0.02  -0.005 -0.043  0.24  -0.021  0.191 -0.037  1.152  0.361  0.06   0.001  0.084 -0.041  0.316  0.238  0.108 -0.032]\n",
      "[1.98  0.443 1.102 0.639 0.552 0.357 1.539 1.934 0.396 0.432 1.126 0.454 1.237 1.729 0.412 1.353 0.693 0.363 0.68  2.809 0.509 1.331 0.506 1.139 2.066 0.549 0.566 0.655 0.683 1.501 1.067 0.703 0.728\n",
      " 2.077 1.977 1.138 0.394 1.188 0.349 1.477 1.122 0.349 0.77  1.988 0.805 1.397 0.369 0.448 0.926 1.772]\n",
      "[ 0.01   0.356 -0.007  0.092  0.094  0.241 -0.004  0.011  0.125  0.364  0.05   0.399  0.046  0.02   0.311  0.054  0.076  0.294 -0.019 -0.009  0.413  0.025  0.202  0.05   0.02   0.513  0.391  0.644\n",
      "  0.075  1.284 -0.006  0.107 -0.027 -0.002  0.007 -0.023  0.25  -0.003  0.162 -0.008  1.079  0.277  0.073  0.012  0.119 -0.013  0.238  0.182  0.002 -0.009]\n",
      "[0.547 0.331 0.587 0.42  0.576 0.257 0.695 0.979 0.24  0.359 0.62  0.376 0.68  0.639 0.285 0.505 0.526 0.295 0.331 0.146 0.387 0.88  0.288 0.477 0.898 0.493 0.383 0.645 0.633 1.256 0.449 0.397 0.272\n",
      " 0.614 1.571 0.26  0.269 0.691 0.229 0.429 1.087 0.269 0.376 0.744 0.279 0.266 0.255 0.286 1.639 0.405]\n",
      "[ 0.02   0.297  0.003  0.118  0.094  0.221  0.01   0.016  0.141  0.34   0.075  0.339  0.054  0.029  0.234  0.084  0.084  0.269 -0.004 -0.011  0.337  0.038  0.173  0.076  0.027  0.478  0.362  0.638\n",
      "  0.069  1.19   0.005  0.129 -0.012  0.008  0.041 -0.011  0.229  0.003  0.161 -0.002  1.081  0.233  0.082  0.021  0.147 -0.006  0.191  0.161  0.024  0.006]\n",
      "[0.396 0.293 0.664 0.198 0.273 0.218 0.539 0.706 0.166 0.341 0.279 0.331 0.36  0.444 0.234 0.243 0.249 0.279 0.349 0.618 0.331 0.441 0.196 0.207 0.611 0.476 0.354 0.649 0.312 1.151 0.337 0.214 0.252\n",
      " 0.473 0.577 0.338 0.227 0.495 0.182 0.358 1.095 0.232 0.204 0.545 0.18  0.41  0.201 0.198 0.425 0.699]\n",
      "[ 0.024  0.288  0.007  0.134  0.108  0.204  0.022  0.016  0.142  0.337  0.092  0.32   0.059  0.039  0.22   0.168  0.088  0.273  0.002 -0.002  0.325  0.043  0.156  0.085  0.031  0.482  0.349  0.653\n",
      "  0.078  1.119  0.007  0.15  -0.002  0.008  0.086 -0.003  0.214  0.006  0.17  -0.002  1.093  0.218  0.091  0.027  0.158  0.001  0.173  0.144  0.031  0.01 ]\n",
      "[0.278 0.286 0.557 0.15  0.159 0.203 0.265 0.584 0.151 0.34  0.16  0.321 0.204 0.256 0.222 0.192 0.153 0.277 0.301 0.156 0.326 0.261 0.16  0.132 0.389 0.499 0.351 0.657 0.165 1.054 0.261 0.166 0.212\n",
      " 0.432 0.169 0.259 0.211 0.339 0.176 0.221 1.1   0.22  0.131 0.349 0.169 0.37  0.171 0.154 0.243 0.596]\n",
      "[0.033 0.283 0.009 0.137 0.119 0.199 0.028 0.018 0.15  0.342 0.101 0.318 0.064 0.044 0.224 0.205 0.093 0.283 0.004 0.001 0.324 0.048 0.152 0.092 0.035 0.506 0.361 0.657 0.083 1.041 0.008 0.151 0.001\n",
      " 0.009 0.103 0.002 0.207 0.005 0.174 0.002 1.131 0.214 0.096 0.031 0.18  0.004 0.162 0.135 0.037 0.015]\n",
      "[0.14  0.295 0.473 0.138 0.13  0.197 0.163 0.414 0.154 0.347 0.119 0.317 0.123 0.147 0.225 0.222 0.111 0.295 0.255 0.234 0.328 0.152 0.154 0.106 0.229 0.51  0.362 0.66  0.11  1.051 0.208 0.155 0.197\n",
      " 0.354 0.121 0.228 0.208 0.294 0.177 0.2   1.166 0.216 0.105 0.204 0.188 0.319 0.16  0.136 0.135 0.38 ]\n",
      "[0.038 0.297 0.014 0.135 0.126 0.196 0.032 0.022 0.157 0.352 0.107 0.32  0.069 0.049 0.223 0.239 0.093 0.299 0.005 0.006 0.33  0.051 0.154 0.097 0.035 0.512 0.367 0.66  0.081 1.059 0.01  0.153 0.002\n",
      " 0.014 0.108 0.004 0.204 0.005 0.197 0.005 1.182 0.212 0.094 0.037 0.192 0.005 0.154 0.13  0.043 0.017]\n",
      "[0.083 0.3   0.278 0.137 0.129 0.197 0.095 0.226 0.161 0.364 0.112 0.322 0.086 0.093 0.222 0.276 0.097 0.303 0.219 0.113 0.334 0.096 0.157 0.101 0.149 0.536 0.391 0.661 0.085 1.035 0.145 0.156 0.177\n",
      " 0.191 0.112 0.176 0.204 0.248 0.206 0.118 1.176 0.212 0.096 0.113 0.22  0.28  0.155 0.132 0.079 0.248]\n",
      "[0.038 0.297 0.016 0.138 0.132 0.198 0.035 0.025 0.166 0.367 0.113 0.324 0.071 0.056 0.225 0.292 0.092 0.311 0.01  0.008 0.337 0.056 0.158 0.099 0.038 0.543 0.398 0.66  0.078 1.036 0.013 0.165 0.006\n",
      " 0.016 0.107 0.005 0.204 0.006 0.206 0.005 1.179 0.212 0.092 0.04  0.235 0.006 0.143 0.129 0.045 0.02 ]\n",
      "[0.061 0.299 0.166 0.14  0.136 0.2   0.062 0.121 0.17  0.368 0.12  0.325 0.075 0.068 0.228 0.307 0.093 0.306 0.105 0.086 0.342 0.069 0.149 0.104 0.086 0.554 0.4   0.66  0.075 1.095 0.087 0.171 0.093\n",
      " 0.127 0.107 0.147 0.205 0.179 0.209 0.096 1.214 0.213 0.094 0.071 0.244 0.219 0.139 0.131 0.058 0.128]\n",
      "[0.037 0.301 0.019 0.141 0.138 0.201 0.037 0.026 0.172 0.374 0.116 0.326 0.071 0.059 0.228 0.319 0.092 0.307 0.013 0.011 0.344 0.058 0.15  0.105 0.042 0.56  0.389 0.661 0.071 1.106 0.015 0.18  0.008\n",
      " 0.018 0.102 0.006 0.205 0.006 0.213 0.005 1.223 0.217 0.093 0.042 0.256 0.007 0.141 0.131 0.047 0.021]\n",
      "[0.045 0.307 0.084 0.142 0.141 0.205 0.046 0.073 0.176 0.378 0.116 0.329 0.072 0.062 0.227 0.33  0.093 0.316 0.057 0.054 0.345 0.06  0.152 0.106 0.057 0.559 0.393 0.662 0.072 1.118 0.057 0.189 0.056\n",
      " 0.072 0.103 0.103 0.207 0.126 0.221 0.075 1.236 0.221 0.094 0.052 0.269 0.162 0.141 0.131 0.05  0.077]\n",
      "[0.036 0.31  0.02  0.143 0.143 0.209 0.039 0.029 0.175 0.386 0.123 0.336 0.071 0.062 0.226 0.342 0.094 0.317 0.015 0.012 0.342 0.058 0.147 0.107 0.044 0.568 0.394 0.661 0.067 1.158 0.016 0.195 0.01\n",
      " 0.019 0.099 0.007 0.207 0.006 0.222 0.005 1.24  0.222 0.094 0.044 0.284 0.008 0.141 0.131 0.047 0.021]\n",
      "[0.039 0.309 0.049 0.143 0.145 0.209 0.041 0.045 0.179 0.391 0.13  0.343 0.072 0.063 0.218 0.354 0.095 0.324 0.037 0.034 0.346 0.059 0.145 0.104 0.048 0.569 0.396 0.666 0.067 1.178 0.037 0.215 0.035\n",
      " 0.045 0.102 0.073 0.209 0.088 0.224 0.054 1.245 0.221 0.094 0.046 0.293 0.095 0.14  0.134 0.048 0.048]\n",
      "[0.036 0.315 0.022 0.143 0.147 0.208 0.039 0.032 0.183 0.41  0.132 0.344 0.072 0.063 0.217 0.358 0.095 0.327 0.015 0.014 0.343 0.059 0.145 0.105 0.046 0.578 0.396 0.667 0.066 1.183 0.018 0.225 0.012\n",
      " 0.02  0.101 0.007 0.208 0.006 0.225 0.006 1.255 0.223 0.095 0.044 0.298 0.008 0.138 0.135 0.048 0.023]\n",
      "[0.038 0.319 0.032 0.144 0.148 0.209 0.04  0.035 0.18  0.415 0.137 0.347 0.071 0.063 0.216 0.363 0.096 0.329 0.026 0.023 0.347 0.059 0.144 0.105 0.047 0.588 0.399 0.668 0.067 1.223 0.025 0.243 0.023\n",
      " 0.031 0.098 0.048 0.208 0.058 0.227 0.038 1.257 0.224 0.095 0.045 0.314 0.064 0.137 0.136 0.05  0.032]\n",
      "[0.036 0.324 0.024 0.145 0.149 0.21  0.04  0.033 0.181 0.42  0.14  0.349 0.072 0.063 0.216 0.374 0.096 0.331 0.016 0.014 0.347 0.059 0.143 0.105 0.046 0.594 0.399 0.669 0.064 1.241 0.018 0.263 0.012\n",
      " 0.021 0.091 0.007 0.209 0.006 0.228 0.006 1.265 0.226 0.096 0.044 0.319 0.008 0.137 0.136 0.05  0.024]\n",
      "[0.031 0.332 0.026 0.147 0.151 0.212 0.037 0.034 0.185 0.428 0.148 0.35  0.072 0.063 0.214 0.381 0.096 0.332 0.02  0.018 0.349 0.059 0.143 0.106 0.046 0.594 0.399 0.673 0.063 1.24  0.02  0.279 0.017\n",
      " 0.024 0.092 0.032 0.209 0.035 0.231 0.023 1.264 0.227 0.099 0.046 0.323 0.041 0.136 0.139 0.051 0.026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.031 0.341 0.024 0.148 0.152 0.211 0.038 0.034 0.193 0.43  0.152 0.357 0.073 0.063 0.214 0.385 0.097 0.336 0.017 0.015 0.348 0.058 0.143 0.108 0.046 0.605 0.401 0.674 0.063 1.266 0.018 0.283 0.013\n",
      " 0.021 0.088 0.007 0.209 0.007 0.234 0.006 1.278 0.227 0.1   0.045 0.329 0.009 0.133 0.14  0.051 0.024]\n",
      "[0.031 0.341 0.025 0.148 0.153 0.214 0.038 0.034 0.197 0.442 0.151 0.359 0.073 0.063 0.215 0.479 0.097 0.34  0.017 0.015 0.346 0.059 0.143 0.108 0.047 0.609 0.403 0.675 0.061 1.248 0.019 0.298 0.015\n",
      " 0.022 0.088 0.021 0.206 0.021 0.246 0.015 1.281 0.23  0.1   0.046 0.331 0.024 0.132 0.141 0.052 0.024]\n",
      "[0.032 0.343 0.025 0.148 0.157 0.219 0.038 0.034 0.197 0.445 0.152 0.359 0.074 0.063 0.204 0.5   0.098 0.342 0.016 0.015 0.348 0.059 0.135 0.109 0.047 0.611 0.4   0.675 0.057 1.238 0.018 0.319 0.017\n",
      " 0.021 0.092 0.007 0.206 0.007 0.25  0.005 1.274 0.235 0.101 0.047 0.333 0.009 0.133 0.141 0.052 0.023]\n",
      "[0.032 0.347 0.025 0.15  0.159 0.22  0.038 0.034 0.197 0.447 0.156 0.361 0.074 0.063 0.195 0.5   0.098 0.343 0.016 0.015 0.351 0.06  0.134 0.109 0.047 0.612 0.401 0.682 0.057 1.251 0.019 0.331 0.018\n",
      " 0.021 0.091 0.015 0.206 0.014 0.264 0.017 1.273 0.238 0.101 0.048 0.333 0.015 0.134 0.142 0.053 0.023]\n",
      "[0.032 0.349 0.025 0.147 0.161 0.222 0.038 0.034 0.198 0.448 0.158 0.361 0.074 0.064 0.192 0.5   0.1   0.344 0.016 0.015 0.353 0.061 0.134 0.11  0.048 0.612 0.402 0.684 0.056 1.258 0.022 0.344 0.019\n",
      " 0.02  0.091 0.007 0.208 0.007 0.269 0.006 1.267 0.242 0.101 0.048 0.336 0.009 0.134 0.144 0.053 0.023]\n",
      "[0.032 0.349 0.026 0.147 0.161 0.224 0.039 0.034 0.206 0.449 0.158 0.361 0.074 0.064 0.187 0.5   0.1   0.346 0.016 0.015 0.355 0.06  0.135 0.112 0.048 0.616 0.403 0.688 0.056 1.261 0.022 0.351 0.02\n",
      " 0.02  0.091 0.011 0.21  0.01  0.275 0.01  1.273 0.244 0.102 0.049 0.34  0.012 0.134 0.148 0.054 0.023]\n",
      "[0.032 0.349 0.026 0.147 0.163 0.223 0.039 0.034 0.204 0.453 0.159 0.363 0.074 0.063 0.187 0.5   0.1   0.346 0.017 0.015 0.356 0.062 0.136 0.113 0.049 0.624 0.403 0.689 0.057 1.264 0.024 0.359 0.02\n",
      " 0.02  0.091 0.008 0.209 0.007 0.281 0.006 1.276 0.245 0.102 0.049 0.34  0.01  0.134 0.149 0.055 0.023]\n",
      "[0.032 0.353 0.026 0.147 0.166 0.222 0.039 0.034 0.206 0.455 0.162 0.366 0.074 0.064 0.185 0.5   0.1   0.351 0.018 0.015 0.357 0.062 0.137 0.113 0.049 0.62  0.404 0.691 0.058 1.268 0.026 0.36  0.02\n",
      " 0.02  0.092 0.01  0.207 0.009 0.289 0.008 1.284 0.247 0.102 0.05  0.339 0.011 0.134 0.149 0.056 0.023]\n",
      "[0.032 0.359 0.026 0.147 0.168 0.223 0.039 0.034 0.211 0.457 0.163 0.366 0.074 0.064 0.185 0.5   0.1   0.351 0.019 0.015 0.357 0.062 0.138 0.114 0.049 0.62  0.404 0.691 0.058 1.278 0.028 0.363 0.021\n",
      " 0.02  0.093 0.008 0.206 0.008 0.289 0.007 1.287 0.248 0.102 0.05  0.341 0.01  0.133 0.149 0.057 0.023]\n",
      "[0.032 0.36  0.026 0.148 0.169 0.225 0.04  0.035 0.219 0.458 0.163 0.369 0.075 0.064 0.181 0.5   0.1   0.351 0.02  0.015 0.359 0.062 0.138 0.115 0.049 0.629 0.405 0.691 0.059 1.279 0.028 0.364 0.021\n",
      " 0.021 0.094 0.009 0.202 0.008 0.289 0.008 1.288 0.249 0.103 0.05  0.341 0.01  0.133 0.15  0.057 0.023]\n",
      "[0.032 0.36  0.026 0.148 0.169 0.226 0.04  0.035 0.221 0.466 0.163 0.369 0.073 0.064 0.182 0.5   0.1   0.35  0.021 0.015 0.36  0.062 0.138 0.116 0.05  0.636 0.409 0.696 0.059 1.281 0.029 0.367 0.021\n",
      " 0.021 0.095 0.009 0.2   0.008 0.29  0.007 1.294 0.249 0.103 0.051 0.341 0.01  0.132 0.151 0.058 0.023]\n",
      "[0.032 0.362 0.027 0.15  0.168 0.222 0.041 0.035 0.229 0.467 0.164 0.37  0.073 0.065 0.182 0.5   0.101 0.354 0.022 0.015 0.363 0.062 0.139 0.116 0.05  0.642 0.408 0.696 0.059 1.283 0.03  0.367 0.021\n",
      " 0.021 0.099 0.009 0.199 0.008 0.29  0.008 1.297 0.249 0.103 0.051 0.342 0.01  0.13  0.151 0.059 0.023]\n",
      "[0.032 0.361 0.027 0.151 0.17  0.218 0.04  0.035 0.229 0.471 0.164 0.372 0.074 0.065 0.185 0.5   0.102 0.355 0.023 0.015 0.364 0.063 0.139 0.116 0.051 0.644 0.408 0.696 0.059 1.285 0.031 0.369 0.022\n",
      " 0.021 0.099 0.009 0.196 0.008 0.29  0.007 1.299 0.249 0.103 0.051 0.351 0.01  0.13  0.151 0.059 0.023]\n",
      "[0.032 0.36  0.027 0.152 0.171 0.219 0.04  0.035 0.23  0.474 0.165 0.373 0.074 0.065 0.184 0.5   0.102 0.355 0.024 0.016 0.365 0.065 0.14  0.117 0.051 0.646 0.407 0.696 0.062 1.313 0.031 0.37  0.023\n",
      " 0.021 0.092 0.009 0.196 0.008 0.29  0.008 1.303 0.249 0.103 0.052 0.353 0.01  0.13  0.151 0.06  0.023]\n",
      "[0.032 0.362 0.027 0.152 0.17  0.22  0.04  0.036 0.231 0.478 0.166 0.372 0.074 0.066 0.184 0.5   0.101 0.352 0.024 0.016 0.367 0.066 0.14  0.117 0.051 0.648 0.408 0.696 0.059 1.328 0.032 0.379 0.023\n",
      " 0.021 0.093 0.009 0.195 0.008 0.29  0.008 1.301 0.25  0.103 0.053 0.353 0.01  0.129 0.151 0.06  0.023]\n",
      "[0.032 0.364 0.027 0.152 0.17  0.221 0.04  0.036 0.232 0.478 0.168 0.372 0.073 0.066 0.183 0.5   0.103 0.355 0.025 0.016 0.368 0.066 0.14  0.118 0.051 0.648 0.408 0.699 0.06  1.33  0.032 0.386 0.024\n",
      " 0.021 0.094 0.009 0.195 0.008 0.29  0.008 1.303 0.251 0.104 0.053 0.354 0.011 0.13  0.151 0.06  0.023]\n",
      "[0.032 0.364 0.027 0.153 0.169 0.221 0.041 0.035 0.233 0.48  0.168 0.373 0.073 0.066 0.175 0.5   0.103 0.357 0.025 0.016 0.37  0.066 0.141 0.121 0.052 0.648 0.408 0.699 0.06  1.353 0.032 0.391 0.026\n",
      " 0.021 0.092 0.009 0.195 0.008 0.29  0.008 1.3   0.252 0.104 0.053 0.355 0.011 0.13  0.152 0.06  0.023]\n",
      "[0.032 0.365 0.027 0.154 0.171 0.224 0.041 0.036 0.233 0.48  0.176 0.373 0.073 0.066 0.173 0.5   0.103 0.357 0.027 0.016 0.37  0.066 0.141 0.121 0.052 0.648 0.405 0.698 0.061 1.358 0.032 0.391 0.029\n",
      " 0.021 0.094 0.009 0.195 0.008 0.291 0.008 1.3   0.255 0.105 0.054 0.358 0.011 0.13  0.152 0.06  0.023]\n",
      "[0.032 0.366 0.027 0.154 0.171 0.224 0.041 0.036 0.235 0.483 0.177 0.374 0.074 0.066 0.171 0.5   0.103 0.358 0.027 0.016 0.37  0.066 0.14  0.122 0.052 0.649 0.405 0.7   0.062 1.358 0.033 0.391 0.034\n",
      " 0.021 0.104 0.009 0.195 0.008 0.291 0.008 1.3   0.256 0.106 0.055 0.358 0.011 0.13  0.152 0.061 0.023]\n",
      "[0.033 0.367 0.027 0.155 0.171 0.224 0.041 0.036 0.235 0.484 0.178 0.375 0.072 0.066 0.169 0.5   0.103 0.36  0.028 0.016 0.37  0.067 0.141 0.123 0.052 0.649 0.405 0.703 0.062 1.366 0.033 0.391 0.036\n",
      " 0.021 0.104 0.009 0.196 0.008 0.291 0.008 1.307 0.256 0.107 0.057 0.358 0.011 0.131 0.152 0.061 0.023]\n",
      "[0.033 0.367 0.027 0.156 0.172 0.225 0.041 0.036 0.235 0.487 0.18  0.376 0.072 0.067 0.167 0.5   0.103 0.36  0.029 0.017 0.371 0.067 0.141 0.126 0.052 0.65  0.405 0.703 0.063 1.366 0.033 0.391 0.037\n",
      " 0.021 0.108 0.009 0.197 0.008 0.292 0.008 1.308 0.258 0.107 0.056 0.358 0.011 0.131 0.152 0.061 0.023]\n",
      "[0.033 0.368 0.027 0.157 0.172 0.227 0.041 0.036 0.236 0.487 0.18  0.377 0.072 0.067 0.148 0.501 0.104 0.36  0.03  0.017 0.371 0.068 0.141 0.13  0.052 0.65  0.405 0.704 0.063 1.414 0.033 0.392 0.038\n",
      " 0.021 0.109 0.009 0.197 0.008 0.292 0.008 1.307 0.259 0.107 0.057 0.357 0.011 0.131 0.152 0.06  0.023]\n",
      "[0.033 0.369 0.027 0.157 0.173 0.228 0.041 0.036 0.236 0.491 0.181 0.377 0.07  0.067 0.151 0.501 0.104 0.357 0.03  0.017 0.375 0.068 0.142 0.132 0.052 0.655 0.405 0.704 0.064 1.424 0.033 0.392 0.042\n",
      " 0.021 0.108 0.009 0.197 0.008 0.292 0.008 1.315 0.26  0.108 0.057 0.357 0.011 0.131 0.152 0.059 0.023]\n",
      "[0.033 0.371 0.027 0.158 0.174 0.227 0.041 0.036 0.236 0.494 0.181 0.377 0.071 0.068 0.156 0.5   0.104 0.357 0.03  0.017 0.377 0.068 0.142 0.133 0.052 0.653 0.403 0.704 0.063 1.449 0.033 0.392 0.043\n",
      " 0.021 0.108 0.009 0.198 0.008 0.292 0.008 1.32  0.259 0.109 0.057 0.357 0.011 0.129 0.152 0.06  0.023]\n",
      "[0.033 0.372 0.027 0.158 0.174 0.227 0.041 0.036 0.236 0.495 0.183 0.379 0.071 0.068 0.169 0.501 0.103 0.359 0.031 0.017 0.377 0.068 0.145 0.133 0.053 0.654 0.404 0.705 0.061 1.442 0.034 0.392 0.043\n",
      " 0.021 0.109 0.009 0.199 0.008 0.292 0.008 1.322 0.259 0.11  0.057 0.357 0.011 0.131 0.15  0.06  0.022]\n",
      "[0.033 0.372 0.027 0.158 0.176 0.227 0.041 0.037 0.235 0.497 0.184 0.379 0.071 0.068 0.172 0.501 0.103 0.359 0.032 0.018 0.377 0.068 0.145 0.134 0.053 0.652 0.406 0.705 0.062 1.441 0.034 0.392 0.044\n",
      " 0.022 0.111 0.009 0.204 0.008 0.292 0.008 1.321 0.259 0.112 0.058 0.356 0.011 0.131 0.151 0.061 0.023]\n",
      "[0.033 0.372 0.027 0.158 0.176 0.229 0.041 0.037 0.235 0.497 0.187 0.379 0.071 0.068 0.173 0.501 0.103 0.363 0.034 0.018 0.377 0.068 0.144 0.139 0.053 0.652 0.407 0.705 0.063 1.442 0.034 0.391 0.045\n",
      " 0.02  0.113 0.009 0.205 0.008 0.292 0.008 1.321 0.26  0.116 0.058 0.356 0.011 0.131 0.151 0.062 0.023]\n",
      "[0.033 0.373 0.028 0.159 0.176 0.229 0.041 0.037 0.235 0.497 0.189 0.38  0.071 0.069 0.176 0.5   0.104 0.363 0.037 0.018 0.378 0.067 0.145 0.142 0.054 0.646 0.401 0.705 0.066 1.441 0.034 0.389 0.053\n",
      " 0.018 0.112 0.009 0.213 0.008 0.291 0.008 1.321 0.261 0.116 0.059 0.364 0.011 0.131 0.152 0.062 0.023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.033 0.365 0.028 0.159 0.176 0.23  0.041 0.037 0.235 0.498 0.192 0.383 0.071 0.069 0.178 0.5   0.105 0.363 0.038 0.018 0.379 0.067 0.145 0.147 0.054 0.641 0.403 0.705 0.077 1.438 0.034 0.389 0.057\n",
      " 0.017 0.111 0.009 0.213 0.008 0.291 0.009 1.324 0.262 0.116 0.06  0.364 0.011 0.13  0.152 0.061 0.023]\n",
      "[0.033 0.356 0.028 0.16  0.176 0.231 0.041 0.037 0.236 0.497 0.193 0.384 0.071 0.069 0.179 0.5   0.106 0.361 0.039 0.018 0.379 0.067 0.145 0.15  0.054 0.641 0.403 0.705 0.089 1.438 0.035 0.391 0.058\n",
      " 0.017 0.111 0.009 0.217 0.008 0.291 0.009 1.323 0.262 0.116 0.06  0.364 0.011 0.129 0.152 0.061 0.023]\n",
      "[0.033 0.356 0.027 0.161 0.177 0.231 0.041 0.037 0.235 0.497 0.194 0.384 0.072 0.069 0.179 0.5   0.106 0.361 0.043 0.018 0.38  0.067 0.145 0.153 0.054 0.639 0.403 0.705 0.098 1.439 0.035 0.392 0.06\n",
      " 0.017 0.112 0.009 0.217 0.008 0.291 0.009 1.322 0.262 0.116 0.06  0.366 0.011 0.129 0.153 0.062 0.023]\n",
      "[0.034 0.377 0.027 0.161 0.177 0.231 0.041 0.037 0.235 0.497 0.195 0.386 0.072 0.069 0.179 0.501 0.107 0.364 0.045 0.018 0.38  0.067 0.145 0.155 0.054 0.638 0.403 0.706 0.11  1.447 0.036 0.392 0.061\n",
      " 0.017 0.108 0.009 0.218 0.008 0.291 0.009 1.304 0.262 0.116 0.06  0.374 0.011 0.129 0.154 0.062 0.023]\n",
      "[0.034 0.396 0.027 0.161 0.177 0.232 0.042 0.038 0.235 0.497 0.196 0.387 0.072 0.069 0.179 0.501 0.106 0.364 0.046 0.018 0.38  0.067 0.145 0.168 0.054 0.636 0.394 0.706 0.111 1.449 0.036 0.393 0.063\n",
      " 0.017 0.107 0.009 0.219 0.008 0.291 0.009 1.285 0.262 0.117 0.06  0.375 0.011 0.129 0.154 0.062 0.023]\n",
      "[0.034 0.392 0.027 0.162 0.177 0.233 0.042 0.038 0.235 0.499 0.196 0.387 0.073 0.07  0.179 0.5   0.107 0.364 0.047 0.018 0.38  0.067 0.145 0.178 0.054 0.635 0.386 0.707 0.112 1.453 0.036 0.396 0.07\n",
      " 0.017 0.107 0.009 0.219 0.008 0.291 0.009 1.294 0.263 0.117 0.061 0.38  0.011 0.128 0.156 0.062 0.023]\n",
      "[0.034 0.396 0.028 0.163 0.177 0.234 0.042 0.038 0.234 0.5   0.196 0.387 0.073 0.069 0.179 0.5   0.107 0.365 0.047 0.018 0.38  0.067 0.145 0.18  0.054 0.635 0.386 0.707 0.117 1.457 0.037 0.396 0.08\n",
      " 0.017 0.108 0.009 0.22  0.008 0.291 0.009 1.295 0.263 0.116 0.06  0.372 0.011 0.128 0.156 0.062 0.023]\n",
      "[0.034 0.391 0.028 0.163 0.177 0.235 0.042 0.038 0.234 0.499 0.196 0.387 0.073 0.07  0.178 0.508 0.107 0.364 0.048 0.018 0.38  0.067 0.145 0.181 0.053 0.648 0.387 0.707 0.133 1.441 0.037 0.396 0.085\n",
      " 0.017 0.108 0.009 0.22  0.008 0.291 0.009 1.297 0.263 0.117 0.06  0.372 0.011 0.128 0.156 0.062 0.023]\n",
      "[0.034 0.395 0.028 0.163 0.177 0.235 0.042 0.036 0.234 0.508 0.196 0.387 0.074 0.07  0.178 0.508 0.107 0.361 0.049 0.018 0.38  0.067 0.144 0.192 0.053 0.648 0.388 0.708 0.138 1.441 0.037 0.395 0.087\n",
      " 0.017 0.109 0.009 0.22  0.008 0.291 0.009 1.297 0.263 0.117 0.06  0.373 0.011 0.128 0.157 0.062 0.024]\n",
      "[0.034 0.401 0.028 0.162 0.177 0.238 0.042 0.037 0.234 0.51  0.197 0.387 0.074 0.07  0.178 0.501 0.107 0.361 0.05  0.018 0.38  0.067 0.144 0.198 0.053 0.656 0.388 0.708 0.147 1.446 0.037 0.396 0.088\n",
      " 0.017 0.109 0.01  0.216 0.008 0.291 0.009 1.29  0.263 0.117 0.06  0.373 0.012 0.125 0.157 0.062 0.023]\n",
      "[0.034 0.412 0.028 0.162 0.177 0.24  0.042 0.037 0.234 0.511 0.197 0.388 0.071 0.069 0.178 0.501 0.107 0.36  0.051 0.018 0.381 0.068 0.144 0.205 0.053 0.656 0.388 0.708 0.148 1.447 0.037 0.396 0.089\n",
      " 0.017 0.109 0.01  0.216 0.008 0.291 0.01  1.282 0.264 0.116 0.061 0.373 0.012 0.124 0.157 0.062 0.023]\n",
      "[0.034 0.419 0.028 0.162 0.178 0.24  0.042 0.037 0.234 0.511 0.198 0.389 0.071 0.07  0.179 0.501 0.108 0.36  0.051 0.018 0.382 0.068 0.144 0.209 0.054 0.658 0.389 0.709 0.149 1.447 0.038 0.396 0.09\n",
      " 0.017 0.109 0.01  0.214 0.008 0.291 0.01  1.275 0.264 0.116 0.061 0.373 0.012 0.124 0.157 0.062 0.023]\n",
      "[0.035 0.425 0.028 0.162 0.178 0.241 0.042 0.037 0.234 0.509 0.198 0.39  0.063 0.07  0.179 0.501 0.109 0.36  0.052 0.019 0.382 0.068 0.144 0.222 0.055 0.662 0.389 0.709 0.15  1.448 0.038 0.398 0.091\n",
      " 0.017 0.109 0.01  0.214 0.008 0.291 0.01  1.269 0.264 0.116 0.062 0.373 0.012 0.124 0.157 0.062 0.023]\n",
      "[0.036 0.431 0.028 0.162 0.178 0.241 0.042 0.037 0.234 0.509 0.198 0.392 0.062 0.07  0.179 0.501 0.109 0.36  0.055 0.019 0.383 0.068 0.144 0.223 0.055 0.662 0.39  0.709 0.154 1.45  0.038 0.402 0.091\n",
      " 0.016 0.109 0.009 0.215 0.008 0.291 0.01  1.271 0.263 0.116 0.063 0.373 0.012 0.122 0.157 0.063 0.023]\n",
      "[0.037 0.433 0.028 0.163 0.178 0.24  0.042 0.037 0.234 0.51  0.198 0.395 0.063 0.07  0.179 0.501 0.11  0.359 0.057 0.019 0.388 0.068 0.144 0.229 0.055 0.663 0.39  0.71  0.157 1.45  0.039 0.402 0.092\n",
      " 0.017 0.109 0.01  0.215 0.008 0.291 0.01  1.27  0.263 0.116 0.063 0.371 0.012 0.122 0.158 0.063 0.023]\n",
      "[0.036 0.445 0.028 0.164 0.178 0.241 0.042 0.037 0.234 0.511 0.198 0.399 0.063 0.071 0.179 0.501 0.11  0.36  0.058 0.019 0.389 0.069 0.144 0.231 0.055 0.663 0.39  0.71  0.157 1.452 0.038 0.404 0.092\n",
      " 0.017 0.109 0.009 0.211 0.007 0.291 0.01  1.276 0.263 0.116 0.063 0.375 0.012 0.12  0.159 0.063 0.023]\n",
      "[0.037 0.461 0.028 0.167 0.177 0.241 0.042 0.038 0.234 0.511 0.198 0.402 0.063 0.071 0.179 0.501 0.11  0.36  0.059 0.019 0.389 0.068 0.144 0.238 0.055 0.646 0.39  0.711 0.157 1.453 0.039 0.41  0.093\n",
      " 0.017 0.109 0.01  0.217 0.008 0.292 0.011 1.27  0.26  0.116 0.063 0.375 0.012 0.119 0.159 0.064 0.023]\n",
      "[0.037 0.461 0.028 0.168 0.178 0.241 0.04  0.038 0.234 0.511 0.198 0.406 0.063 0.071 0.179 0.501 0.11  0.36  0.061 0.019 0.389 0.069 0.144 0.239 0.056 0.647 0.39  0.713 0.158 1.453 0.039 0.415 0.093\n",
      " 0.017 0.109 0.01  0.219 0.007 0.291 0.012 1.265 0.26  0.116 0.063 0.375 0.012 0.119 0.157 0.064 0.023]\n",
      "[0.037 0.465 0.028 0.172 0.176 0.241 0.04  0.038 0.234 0.511 0.198 0.408 0.063 0.071 0.18  0.501 0.11  0.36  0.061 0.02  0.39  0.069 0.144 0.242 0.056 0.646 0.388 0.713 0.158 1.458 0.039 0.415 0.094\n",
      " 0.017 0.109 0.01  0.219 0.008 0.291 0.012 1.264 0.26  0.116 0.066 0.375 0.011 0.12  0.157 0.064 0.023]\n",
      "[0.037 0.465 0.028 0.172 0.176 0.242 0.04  0.038 0.234 0.511 0.197 0.408 0.062 0.071 0.18  0.501 0.11  0.36  0.062 0.02  0.391 0.069 0.145 0.243 0.056 0.646 0.388 0.716 0.162 1.459 0.039 0.416 0.094\n",
      " 0.017 0.11  0.01  0.219 0.008 0.291 0.012 1.264 0.259 0.116 0.066 0.375 0.011 0.12  0.158 0.064 0.023]\n",
      "[0.037 0.467 0.028 0.171 0.177 0.243 0.04  0.038 0.234 0.511 0.198 0.408 0.062 0.071 0.18  0.496 0.11  0.361 0.062 0.02  0.391 0.069 0.145 0.242 0.056 0.646 0.388 0.716 0.17  1.459 0.039 0.415 0.094\n",
      " 0.017 0.11  0.01  0.219 0.008 0.291 0.013 1.265 0.258 0.116 0.066 0.37  0.011 0.12  0.158 0.065 0.023]\n",
      "[0.037 0.467 0.028 0.174 0.177 0.244 0.04  0.039 0.234 0.511 0.198 0.406 0.062 0.071 0.18  0.496 0.11  0.361 0.065 0.021 0.39  0.069 0.145 0.243 0.056 0.647 0.388 0.716 0.171 1.459 0.04  0.415 0.094\n",
      " 0.017 0.11  0.01  0.218 0.008 0.291 0.013 1.265 0.259 0.116 0.066 0.37  0.011 0.12  0.158 0.066 0.023]\n",
      "[0.037 0.468 0.028 0.176 0.177 0.245 0.04  0.039 0.234 0.512 0.198 0.406 0.062 0.071 0.18  0.496 0.11  0.361 0.07  0.021 0.391 0.069 0.145 0.245 0.056 0.647 0.385 0.716 0.171 1.459 0.04  0.416 0.095\n",
      " 0.017 0.11  0.01  0.218 0.008 0.291 0.013 1.265 0.259 0.116 0.066 0.37  0.011 0.12  0.158 0.066 0.023]\n",
      "[0.037 0.468 0.028 0.176 0.177 0.246 0.04  0.039 0.234 0.512 0.198 0.406 0.056 0.071 0.18  0.496 0.11  0.361 0.075 0.022 0.392 0.069 0.145 0.263 0.056 0.647 0.385 0.716 0.171 1.457 0.04  0.417 0.096\n",
      " 0.017 0.11  0.01  0.219 0.008 0.291 0.013 1.265 0.259 0.117 0.067 0.37  0.012 0.12  0.158 0.066 0.023]\n",
      "[0.037 0.469 0.028 0.176 0.177 0.246 0.04  0.039 0.234 0.512 0.198 0.407 0.054 0.071 0.179 0.496 0.11  0.361 0.079 0.022 0.392 0.07  0.145 0.263 0.056 0.647 0.389 0.718 0.171 1.458 0.04  0.417 0.096\n",
      " 0.017 0.11  0.01  0.219 0.008 0.291 0.013 1.271 0.259 0.117 0.067 0.37  0.012 0.12  0.155 0.067 0.023]\n",
      "[0.037 0.473 0.028 0.175 0.178 0.245 0.041 0.039 0.234 0.512 0.2   0.407 0.054 0.069 0.178 0.496 0.11  0.357 0.082 0.022 0.392 0.069 0.145 0.264 0.056 0.648 0.398 0.719 0.172 1.458 0.04  0.418 0.101\n",
      " 0.017 0.11  0.01  0.22  0.008 0.291 0.013 1.278 0.259 0.118 0.067 0.37  0.012 0.12  0.155 0.067 0.022]\n",
      "[0.037 0.474 0.028 0.175 0.178 0.245 0.04  0.039 0.234 0.512 0.201 0.407 0.054 0.069 0.178 0.496 0.108 0.352 0.084 0.022 0.392 0.069 0.145 0.269 0.056 0.648 0.399 0.719 0.172 1.458 0.04  0.418 0.101\n",
      " 0.017 0.11  0.01  0.22  0.008 0.291 0.013 1.283 0.259 0.118 0.067 0.37  0.011 0.12  0.155 0.067 0.022]\n",
      "[0.037 0.475 0.028 0.175 0.178 0.248 0.041 0.039 0.234 0.512 0.201 0.407 0.054 0.069 0.178 0.496 0.109 0.351 0.085 0.022 0.393 0.069 0.145 0.269 0.056 0.648 0.401 0.72  0.172 1.458 0.04  0.418 0.102\n",
      " 0.017 0.11  0.01  0.221 0.008 0.291 0.013 1.282 0.259 0.118 0.066 0.37  0.011 0.12  0.155 0.068 0.022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.037 0.475 0.028 0.176 0.179 0.25  0.041 0.039 0.234 0.513 0.201 0.407 0.054 0.069 0.178 0.496 0.109 0.351 0.085 0.023 0.393 0.069 0.145 0.269 0.056 0.647 0.401 0.719 0.172 1.46  0.04  0.418 0.102\n",
      " 0.017 0.11  0.01  0.22  0.008 0.291 0.013 1.291 0.259 0.118 0.066 0.37  0.011 0.123 0.151 0.068 0.022]\n",
      "[0.037 0.474 0.028 0.175 0.179 0.252 0.041 0.039 0.234 0.513 0.201 0.407 0.054 0.07  0.178 0.496 0.109 0.354 0.085 0.023 0.394 0.069 0.145 0.271 0.056 0.647 0.401 0.72  0.172 1.456 0.04  0.418 0.102\n",
      " 0.017 0.11  0.01  0.22  0.008 0.291 0.013 1.29  0.259 0.118 0.066 0.37  0.011 0.128 0.149 0.068 0.022]\n",
      "[0.037 0.476 0.028 0.175 0.179 0.253 0.041 0.039 0.234 0.513 0.201 0.407 0.054 0.07  0.178 0.496 0.109 0.354 0.086 0.023 0.394 0.069 0.145 0.272 0.056 0.647 0.401 0.72  0.172 1.454 0.041 0.418 0.102\n",
      " 0.017 0.11  0.01  0.22  0.008 0.292 0.013 1.29  0.259 0.118 0.066 0.37  0.011 0.131 0.149 0.069 0.022]\n",
      "[0.037 0.485 0.028 0.175 0.18  0.254 0.041 0.039 0.234 0.514 0.201 0.407 0.054 0.07  0.178 0.496 0.109 0.354 0.086 0.023 0.396 0.069 0.145 0.27  0.056 0.647 0.401 0.72  0.172 1.454 0.041 0.418 0.102\n",
      " 0.017 0.11  0.01  0.22  0.008 0.293 0.013 1.285 0.26  0.118 0.066 0.37  0.011 0.133 0.15  0.069 0.022]\n",
      "[0.037 0.495 0.028 0.175 0.18  0.254 0.041 0.039 0.234 0.514 0.201 0.408 0.055 0.07  0.177 0.496 0.109 0.353 0.089 0.023 0.396 0.069 0.145 0.269 0.057 0.644 0.402 0.72  0.172 1.454 0.041 0.415 0.103\n",
      " 0.017 0.11  0.01  0.22  0.008 0.293 0.013 1.287 0.261 0.118 0.067 0.37  0.011 0.136 0.149 0.069 0.023]\n",
      "[0.038 0.504 0.028 0.175 0.18  0.254 0.041 0.039 0.234 0.514 0.202 0.408 0.055 0.07  0.177 0.496 0.109 0.353 0.093 0.023 0.396 0.07  0.145 0.27  0.057 0.637 0.402 0.72  0.172 1.454 0.041 0.416 0.103\n",
      " 0.017 0.111 0.01  0.22  0.008 0.293 0.014 1.287 0.261 0.118 0.067 0.37  0.011 0.136 0.15  0.069 0.023]\n",
      "[0.037 0.509 0.028 0.175 0.18  0.254 0.041 0.039 0.234 0.514 0.202 0.408 0.055 0.07  0.177 0.496 0.109 0.354 0.1   0.023 0.396 0.069 0.145 0.272 0.056 0.633 0.403 0.72  0.172 1.454 0.041 0.416 0.104\n",
      " 0.017 0.111 0.01  0.221 0.008 0.293 0.014 1.287 0.261 0.118 0.067 0.37  0.011 0.136 0.15  0.069 0.023]\n",
      "[0.038 0.514 0.029 0.175 0.181 0.254 0.041 0.039 0.234 0.515 0.202 0.408 0.056 0.07  0.178 0.496 0.109 0.354 0.102 0.023 0.396 0.069 0.145 0.272 0.056 0.629 0.403 0.72  0.172 1.454 0.041 0.417 0.104\n",
      " 0.017 0.111 0.01  0.221 0.008 0.293 0.014 1.289 0.261 0.118 0.067 0.37  0.011 0.135 0.149 0.07  0.023]\n",
      "[0.037 0.522 0.029 0.175 0.181 0.254 0.041 0.04  0.234 0.515 0.202 0.408 0.056 0.07  0.178 0.496 0.109 0.354 0.103 0.023 0.396 0.069 0.145 0.272 0.057 0.624 0.404 0.721 0.172 1.455 0.041 0.417 0.104\n",
      " 0.017 0.111 0.01  0.223 0.008 0.293 0.014 1.289 0.261 0.118 0.067 0.37  0.011 0.135 0.149 0.07  0.023]\n",
      "[0.038 0.544 0.029 0.175 0.181 0.254 0.041 0.04  0.234 0.514 0.202 0.409 0.056 0.07  0.178 0.496 0.11  0.354 0.104 0.023 0.396 0.068 0.145 0.271 0.058 0.61  0.404 0.72  0.172 1.456 0.041 0.417 0.105\n",
      " 0.017 0.111 0.01  0.223 0.008 0.293 0.014 1.289 0.261 0.118 0.068 0.37  0.011 0.134 0.149 0.071 0.023]\n",
      "[0.038 0.55  0.029 0.175 0.181 0.255 0.041 0.04  0.234 0.509 0.202 0.414 0.056 0.07  0.178 0.496 0.11  0.354 0.106 0.023 0.4   0.068 0.146 0.272 0.058 0.613 0.406 0.72  0.172 1.458 0.042 0.418 0.105\n",
      " 0.016 0.111 0.01  0.223 0.008 0.293 0.014 1.285 0.262 0.118 0.068 0.37  0.011 0.133 0.149 0.071 0.023]\n",
      "[0.038 0.561 0.029 0.175 0.181 0.256 0.041 0.04  0.234 0.507 0.202 0.415 0.057 0.07  0.18  0.496 0.11  0.354 0.107 0.023 0.4   0.068 0.146 0.273 0.058 0.612 0.406 0.72  0.172 1.458 0.042 0.418 0.105\n",
      " 0.016 0.111 0.01  0.223 0.008 0.293 0.014 1.281 0.261 0.118 0.068 0.37  0.011 0.134 0.149 0.071 0.024]\n",
      "[0.037 0.562 0.029 0.175 0.181 0.256 0.042 0.04  0.234 0.507 0.202 0.415 0.057 0.07  0.182 0.496 0.11  0.354 0.108 0.023 0.4   0.068 0.146 0.273 0.058 0.613 0.406 0.72  0.172 1.458 0.043 0.418 0.105\n",
      " 0.016 0.111 0.01  0.224 0.008 0.293 0.014 1.279 0.26  0.118 0.069 0.37  0.011 0.134 0.149 0.07  0.024]\n",
      "[0.038 0.558 0.029 0.175 0.181 0.256 0.042 0.04  0.234 0.507 0.202 0.415 0.057 0.07  0.182 0.496 0.11  0.354 0.108 0.023 0.401 0.068 0.146 0.274 0.058 0.614 0.406 0.72  0.172 1.457 0.043 0.418 0.105\n",
      " 0.016 0.111 0.01  0.224 0.008 0.293 0.014 1.284 0.26  0.118 0.069 0.37  0.011 0.134 0.149 0.071 0.024]\n",
      "[0.038 0.56  0.029 0.175 0.182 0.256 0.042 0.04  0.233 0.507 0.202 0.415 0.057 0.07  0.182 0.496 0.11  0.354 0.108 0.024 0.402 0.069 0.147 0.274 0.058 0.615 0.406 0.72  0.172 1.457 0.043 0.418 0.105\n",
      " 0.016 0.111 0.01  0.224 0.008 0.293 0.014 1.284 0.26  0.118 0.069 0.37  0.012 0.134 0.149 0.071 0.024]\n",
      "[0.038 0.56  0.029 0.175 0.182 0.257 0.042 0.04  0.234 0.507 0.202 0.415 0.057 0.07  0.182 0.496 0.111 0.354 0.108 0.024 0.402 0.069 0.147 0.274 0.058 0.614 0.406 0.72  0.172 1.458 0.044 0.418 0.105\n",
      " 0.016 0.111 0.01  0.224 0.008 0.293 0.014 1.284 0.26  0.118 0.069 0.37  0.012 0.134 0.15  0.071 0.024]\n",
      "[0.038 0.56  0.029 0.176 0.181 0.257 0.042 0.04  0.234 0.509 0.202 0.415 0.057 0.071 0.182 0.496 0.111 0.354 0.108 0.024 0.402 0.069 0.147 0.274 0.058 0.614 0.407 0.72  0.172 1.458 0.044 0.418 0.106\n",
      " 0.016 0.111 0.01  0.224 0.008 0.293 0.014 1.284 0.261 0.118 0.069 0.37  0.012 0.135 0.15  0.071 0.024]\n",
      "[0.038 0.56  0.029 0.176 0.181 0.257 0.042 0.04  0.234 0.509 0.202 0.415 0.057 0.072 0.182 0.496 0.111 0.353 0.109 0.024 0.402 0.069 0.147 0.274 0.058 0.614 0.409 0.72  0.172 1.458 0.044 0.418 0.107\n",
      " 0.016 0.111 0.01  0.224 0.007 0.293 0.014 1.284 0.261 0.118 0.069 0.37  0.012 0.135 0.15  0.072 0.024]\n",
      "[0.038 0.56  0.029 0.177 0.181 0.257 0.042 0.04  0.234 0.509 0.202 0.415 0.057 0.072 0.182 0.496 0.112 0.353 0.109 0.024 0.403 0.069 0.147 0.272 0.058 0.615 0.409 0.72  0.172 1.458 0.044 0.419 0.107\n",
      " 0.016 0.114 0.01  0.224 0.007 0.293 0.014 1.285 0.261 0.118 0.069 0.37  0.012 0.135 0.15  0.071 0.024]\n",
      "[0.038 0.56  0.029 0.177 0.181 0.257 0.042 0.04  0.234 0.518 0.202 0.415 0.057 0.072 0.182 0.496 0.112 0.354 0.109 0.024 0.403 0.069 0.147 0.272 0.058 0.615 0.409 0.72  0.172 1.457 0.044 0.419 0.107\n",
      " 0.016 0.114 0.01  0.224 0.008 0.293 0.014 1.284 0.262 0.118 0.069 0.37  0.012 0.133 0.15  0.072 0.024]\n",
      "[0.038 0.56  0.029 0.178 0.181 0.257 0.042 0.041 0.234 0.521 0.202 0.415 0.057 0.072 0.182 0.496 0.112 0.354 0.11  0.024 0.403 0.069 0.147 0.272 0.058 0.615 0.409 0.72  0.172 1.457 0.045 0.419 0.107\n",
      " 0.016 0.114 0.01  0.223 0.007 0.293 0.014 1.284 0.262 0.118 0.069 0.37  0.012 0.131 0.15  0.072 0.024]\n",
      "[0.038 0.561 0.029 0.178 0.181 0.257 0.042 0.041 0.234 0.521 0.202 0.415 0.057 0.072 0.182 0.496 0.112 0.354 0.11  0.024 0.403 0.069 0.147 0.272 0.058 0.615 0.409 0.72  0.172 1.457 0.045 0.419 0.107\n",
      " 0.016 0.114 0.01  0.223 0.008 0.293 0.014 1.284 0.262 0.118 0.069 0.37  0.012 0.131 0.151 0.072 0.024]\n",
      "[0.038 0.561 0.029 0.178 0.182 0.257 0.042 0.041 0.234 0.522 0.202 0.416 0.056 0.072 0.182 0.496 0.112 0.354 0.11  0.024 0.403 0.069 0.147 0.275 0.058 0.615 0.409 0.72  0.172 1.457 0.045 0.421 0.107\n",
      " 0.016 0.114 0.01  0.224 0.008 0.293 0.014 1.284 0.263 0.119 0.069 0.37  0.012 0.131 0.151 0.072 0.024]\n",
      "[0.038 0.561 0.029 0.178 0.182 0.257 0.041 0.041 0.233 0.522 0.202 0.416 0.056 0.072 0.182 0.496 0.112 0.354 0.11  0.024 0.403 0.069 0.147 0.275 0.058 0.615 0.41  0.72  0.172 1.457 0.045 0.422 0.107\n",
      " 0.016 0.114 0.01  0.224 0.008 0.293 0.014 1.284 0.263 0.119 0.069 0.37  0.012 0.131 0.151 0.072 0.024]\n",
      "[0.038 0.561 0.029 0.178 0.182 0.257 0.041 0.041 0.233 0.523 0.202 0.417 0.056 0.072 0.182 0.496 0.112 0.354 0.11  0.024 0.403 0.069 0.147 0.276 0.058 0.615 0.41  0.72  0.172 1.457 0.045 0.422 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.014 1.284 0.264 0.119 0.068 0.37  0.012 0.131 0.151 0.072 0.024]\n",
      "[0.038 0.561 0.029 0.178 0.182 0.257 0.041 0.041 0.234 0.525 0.202 0.417 0.057 0.072 0.182 0.496 0.111 0.354 0.111 0.025 0.403 0.069 0.147 0.275 0.058 0.615 0.41  0.72  0.172 1.457 0.046 0.422 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.014 1.284 0.264 0.119 0.068 0.37  0.012 0.131 0.151 0.072 0.024]\n",
      "[0.038 0.561 0.029 0.179 0.18  0.257 0.041 0.041 0.234 0.528 0.202 0.417 0.057 0.072 0.182 0.496 0.111 0.354 0.111 0.025 0.402 0.069 0.147 0.275 0.058 0.615 0.41  0.72  0.172 1.457 0.046 0.423 0.107\n",
      " 0.016 0.114 0.01  0.224 0.008 0.293 0.014 1.284 0.264 0.119 0.068 0.37  0.012 0.131 0.151 0.072 0.024]\n",
      "[0.038 0.561 0.029 0.179 0.18  0.257 0.041 0.041 0.234 0.529 0.202 0.416 0.057 0.072 0.182 0.496 0.111 0.354 0.111 0.025 0.402 0.069 0.147 0.275 0.058 0.615 0.41  0.72  0.172 1.458 0.046 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.014 1.285 0.263 0.119 0.069 0.37  0.012 0.131 0.151 0.072 0.024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.038 0.561 0.029 0.179 0.18  0.257 0.041 0.042 0.234 0.529 0.202 0.417 0.057 0.072 0.182 0.496 0.111 0.354 0.111 0.026 0.403 0.069 0.147 0.275 0.058 0.615 0.41  0.721 0.172 1.458 0.046 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.014 1.285 0.264 0.119 0.068 0.369 0.012 0.131 0.151 0.072 0.024]\n",
      "[0.038 0.561 0.029 0.179 0.18  0.257 0.041 0.042 0.234 0.529 0.202 0.417 0.057 0.072 0.182 0.496 0.11  0.354 0.111 0.026 0.403 0.069 0.15  0.275 0.058 0.615 0.41  0.721 0.172 1.458 0.047 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.014 1.285 0.264 0.119 0.069 0.369 0.012 0.131 0.151 0.072 0.024]\n",
      "[0.038 0.561 0.029 0.179 0.182 0.257 0.041 0.042 0.234 0.529 0.202 0.417 0.057 0.072 0.182 0.496 0.11  0.354 0.111 0.026 0.403 0.069 0.151 0.275 0.058 0.615 0.41  0.721 0.172 1.458 0.047 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.014 1.285 0.264 0.119 0.069 0.369 0.012 0.131 0.151 0.073 0.025]\n",
      "[0.038 0.56  0.029 0.178 0.183 0.257 0.041 0.042 0.234 0.529 0.202 0.417 0.057 0.072 0.182 0.496 0.111 0.354 0.111 0.026 0.403 0.069 0.151 0.275 0.058 0.615 0.41  0.721 0.172 1.458 0.047 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.015 1.286 0.264 0.119 0.069 0.369 0.012 0.131 0.151 0.073 0.025]\n",
      "[0.038 0.56  0.029 0.178 0.183 0.257 0.041 0.042 0.237 0.529 0.202 0.417 0.057 0.072 0.182 0.496 0.111 0.354 0.111 0.026 0.403 0.069 0.151 0.275 0.059 0.615 0.41  0.721 0.172 1.458 0.047 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.014 1.286 0.264 0.119 0.069 0.369 0.012 0.131 0.151 0.073 0.025]\n",
      "[0.038 0.558 0.029 0.178 0.183 0.257 0.041 0.042 0.242 0.529 0.202 0.417 0.057 0.072 0.182 0.496 0.11  0.354 0.111 0.026 0.403 0.069 0.151 0.275 0.059 0.615 0.41  0.721 0.172 1.458 0.047 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.014 1.29  0.264 0.119 0.069 0.369 0.012 0.131 0.151 0.073 0.025]\n",
      "[0.038 0.556 0.029 0.178 0.183 0.257 0.041 0.042 0.242 0.529 0.202 0.417 0.057 0.072 0.182 0.496 0.11  0.354 0.111 0.026 0.404 0.069 0.152 0.275 0.059 0.615 0.41  0.721 0.172 1.459 0.047 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.015 1.292 0.264 0.119 0.069 0.369 0.012 0.131 0.151 0.073 0.025]\n",
      "[0.038 0.556 0.029 0.178 0.183 0.257 0.041 0.043 0.242 0.529 0.202 0.416 0.057 0.072 0.182 0.496 0.11  0.354 0.111 0.026 0.404 0.069 0.153 0.275 0.059 0.615 0.41  0.721 0.172 1.457 0.047 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.015 1.293 0.264 0.119 0.068 0.369 0.012 0.132 0.151 0.073 0.025]\n",
      "[0.038 0.556 0.029 0.178 0.183 0.257 0.041 0.043 0.242 0.529 0.202 0.416 0.057 0.072 0.182 0.496 0.11  0.354 0.111 0.027 0.404 0.069 0.154 0.275 0.059 0.615 0.41  0.721 0.172 1.457 0.048 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.015 1.294 0.264 0.119 0.068 0.369 0.012 0.131 0.151 0.073 0.025]\n",
      "[0.038 0.556 0.029 0.178 0.183 0.257 0.041 0.043 0.242 0.53  0.202 0.418 0.057 0.072 0.182 0.496 0.111 0.353 0.111 0.027 0.404 0.069 0.154 0.275 0.059 0.615 0.41  0.721 0.172 1.457 0.048 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.015 1.294 0.265 0.119 0.068 0.369 0.012 0.131 0.151 0.073 0.025]\n",
      "[0.038 0.556 0.029 0.178 0.182 0.257 0.041 0.043 0.242 0.531 0.202 0.422 0.057 0.072 0.182 0.496 0.111 0.353 0.112 0.027 0.404 0.069 0.154 0.275 0.059 0.615 0.409 0.721 0.172 1.457 0.048 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.015 1.294 0.265 0.119 0.068 0.369 0.012 0.128 0.151 0.073 0.025]\n",
      "[0.038 0.556 0.029 0.178 0.182 0.257 0.041 0.044 0.242 0.531 0.202 0.422 0.057 0.072 0.182 0.496 0.111 0.353 0.112 0.027 0.404 0.069 0.154 0.275 0.059 0.615 0.409 0.722 0.172 1.457 0.048 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.015 1.297 0.265 0.121 0.068 0.369 0.012 0.127 0.151 0.073 0.025]\n",
      "[0.038 0.556 0.028 0.178 0.182 0.257 0.041 0.044 0.242 0.53  0.202 0.422 0.057 0.072 0.182 0.496 0.112 0.353 0.112 0.027 0.404 0.069 0.154 0.275 0.059 0.615 0.409 0.722 0.172 1.458 0.049 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.015 1.297 0.265 0.121 0.068 0.369 0.012 0.127 0.151 0.073 0.025]\n",
      "[0.038 0.556 0.028 0.178 0.182 0.257 0.041 0.044 0.242 0.53  0.202 0.427 0.057 0.072 0.182 0.496 0.112 0.353 0.113 0.027 0.404 0.069 0.154 0.277 0.059 0.615 0.41  0.722 0.172 1.458 0.049 0.423 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.016 1.298 0.265 0.121 0.068 0.369 0.012 0.125 0.152 0.074 0.025]\n",
      "[0.038 0.556 0.028 0.178 0.182 0.257 0.041 0.044 0.242 0.53  0.202 0.428 0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.027 0.407 0.069 0.155 0.277 0.059 0.615 0.41  0.722 0.172 1.458 0.049 0.422 0.107\n",
      " 0.016 0.114 0.011 0.224 0.008 0.293 0.016 1.298 0.265 0.121 0.068 0.369 0.012 0.125 0.152 0.074 0.025]\n",
      "[0.038 0.556 0.028 0.178 0.182 0.257 0.041 0.044 0.242 0.53  0.202 0.43  0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.027 0.407 0.069 0.155 0.277 0.059 0.615 0.41  0.722 0.172 1.458 0.049 0.422 0.107\n",
      " 0.016 0.114 0.011 0.225 0.008 0.293 0.016 1.298 0.265 0.121 0.068 0.369 0.012 0.125 0.152 0.074 0.025]\n",
      "[0.038 0.556 0.028 0.178 0.182 0.257 0.041 0.044 0.242 0.53  0.202 0.431 0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.027 0.407 0.069 0.155 0.277 0.059 0.615 0.41  0.722 0.172 1.459 0.049 0.422 0.107\n",
      " 0.016 0.114 0.011 0.225 0.008 0.293 0.016 1.298 0.265 0.121 0.068 0.369 0.012 0.125 0.152 0.074 0.025]\n",
      "[0.038 0.556 0.028 0.178 0.183 0.257 0.041 0.043 0.242 0.53  0.202 0.435 0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.027 0.407 0.069 0.155 0.277 0.059 0.615 0.41  0.722 0.172 1.459 0.049 0.422 0.107\n",
      " 0.016 0.114 0.011 0.225 0.008 0.293 0.016 1.298 0.265 0.121 0.069 0.369 0.012 0.124 0.151 0.074 0.025]\n",
      "[0.038 0.557 0.028 0.178 0.183 0.257 0.041 0.043 0.242 0.53  0.202 0.436 0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.027 0.407 0.069 0.155 0.277 0.059 0.615 0.41  0.722 0.172 1.459 0.049 0.422 0.107\n",
      " 0.016 0.114 0.011 0.225 0.008 0.293 0.016 1.298 0.265 0.121 0.069 0.369 0.012 0.124 0.151 0.074 0.025]\n",
      "[0.038 0.557 0.028 0.179 0.183 0.257 0.041 0.043 0.242 0.53  0.202 0.436 0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.027 0.407 0.069 0.155 0.277 0.059 0.616 0.41  0.722 0.172 1.459 0.049 0.422 0.107\n",
      " 0.016 0.114 0.011 0.225 0.008 0.293 0.016 1.298 0.265 0.121 0.069 0.369 0.012 0.124 0.151 0.074 0.025]\n",
      "[0.038 0.557 0.028 0.181 0.183 0.257 0.041 0.043 0.242 0.53  0.202 0.435 0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.027 0.407 0.069 0.155 0.276 0.059 0.616 0.41  0.722 0.172 1.458 0.049 0.422 0.107\n",
      " 0.016 0.114 0.011 0.225 0.008 0.293 0.016 1.298 0.265 0.121 0.069 0.37  0.012 0.124 0.152 0.076 0.025]\n",
      "[0.038 0.557 0.028 0.181 0.183 0.257 0.041 0.043 0.242 0.531 0.202 0.435 0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.027 0.408 0.068 0.155 0.276 0.06  0.616 0.41  0.722 0.172 1.458 0.05  0.422 0.107\n",
      " 0.016 0.115 0.011 0.225 0.008 0.293 0.016 1.298 0.265 0.121 0.069 0.37  0.012 0.124 0.152 0.076 0.025]\n",
      "[0.038 0.557 0.028 0.181 0.183 0.257 0.041 0.041 0.242 0.532 0.202 0.436 0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.027 0.411 0.068 0.155 0.275 0.06  0.617 0.41  0.722 0.172 1.458 0.05  0.422 0.107\n",
      " 0.016 0.115 0.011 0.225 0.008 0.293 0.016 1.298 0.265 0.121 0.069 0.37  0.012 0.124 0.152 0.076 0.025]\n",
      "[0.038 0.557 0.028 0.181 0.183 0.257 0.041 0.041 0.242 0.532 0.202 0.436 0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.027 0.411 0.069 0.155 0.274 0.06  0.617 0.41  0.722 0.172 1.458 0.05  0.422 0.107\n",
      " 0.016 0.115 0.011 0.225 0.008 0.293 0.016 1.298 0.265 0.121 0.069 0.37  0.012 0.124 0.152 0.078 0.025]\n",
      "[0.038 0.557 0.028 0.181 0.183 0.257 0.041 0.041 0.242 0.532 0.202 0.436 0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.027 0.411 0.069 0.155 0.274 0.06  0.617 0.41  0.722 0.172 1.458 0.05  0.422 0.107\n",
      " 0.016 0.115 0.011 0.225 0.008 0.293 0.016 1.298 0.265 0.121 0.069 0.37  0.012 0.124 0.152 0.078 0.025]\n",
      "[0.038 0.557 0.028 0.181 0.183 0.257 0.041 0.041 0.242 0.532 0.202 0.436 0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.028 0.411 0.069 0.155 0.274 0.06  0.617 0.41  0.722 0.172 1.46  0.05  0.422 0.107\n",
      " 0.016 0.115 0.011 0.225 0.008 0.293 0.016 1.298 0.265 0.121 0.069 0.37  0.012 0.124 0.152 0.078 0.025]\n",
      "[0.038 0.557 0.028 0.181 0.183 0.257 0.041 0.041 0.242 0.532 0.204 0.436 0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.028 0.411 0.069 0.155 0.274 0.06  0.617 0.409 0.722 0.172 1.461 0.05  0.422 0.107\n",
      " 0.016 0.115 0.011 0.225 0.008 0.293 0.017 1.298 0.265 0.121 0.069 0.37  0.012 0.124 0.152 0.078 0.025]\n",
      "[0.038 0.557 0.028 0.181 0.183 0.257 0.041 0.041 0.242 0.532 0.205 0.436 0.057 0.073 0.182 0.496 0.112 0.353 0.113 0.028 0.411 0.069 0.155 0.274 0.06  0.617 0.409 0.723 0.172 1.462 0.05  0.422 0.107\n",
      " 0.016 0.115 0.011 0.225 0.008 0.293 0.017 1.298 0.265 0.121 0.069 0.37  0.012 0.124 0.152 0.078 0.025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.038 0.557 0.028 0.181 0.183 0.258 0.041 0.041 0.242 0.532 0.205 0.436 0.058 0.073 0.182 0.496 0.112 0.353 0.113 0.028 0.411 0.069 0.155 0.274 0.06  0.617 0.409 0.723 0.172 1.462 0.05  0.422 0.107\n",
      " 0.016 0.115 0.011 0.225 0.008 0.293 0.017 1.298 0.265 0.121 0.069 0.37  0.012 0.124 0.152 0.078 0.025]\n",
      "[0.038 0.557 0.028 0.181 0.183 0.258 0.041 0.041 0.242 0.532 0.205 0.438 0.059 0.073 0.182 0.496 0.112 0.353 0.113 0.028 0.411 0.069 0.155 0.274 0.06  0.617 0.409 0.723 0.172 1.462 0.05  0.422 0.107\n",
      " 0.015 0.115 0.011 0.225 0.008 0.293 0.017 1.298 0.265 0.121 0.069 0.37  0.012 0.125 0.152 0.078 0.025]\n",
      "[0.038 0.557 0.028 0.181 0.183 0.258 0.041 0.041 0.242 0.533 0.205 0.438 0.059 0.073 0.182 0.496 0.112 0.353 0.113 0.028 0.411 0.069 0.155 0.274 0.06  0.617 0.41  0.723 0.172 1.462 0.05  0.422 0.107\n",
      " 0.016 0.115 0.011 0.225 0.008 0.293 0.017 1.298 0.265 0.121 0.069 0.37  0.012 0.125 0.152 0.078 0.025]\n",
      "[0.038 0.558 0.028 0.181 0.183 0.258 0.041 0.041 0.242 0.533 0.205 0.438 0.059 0.073 0.182 0.496 0.112 0.353 0.114 0.028 0.411 0.069 0.155 0.274 0.06  0.617 0.41  0.723 0.172 1.462 0.05  0.422 0.107\n",
      " 0.016 0.115 0.011 0.225 0.008 0.293 0.017 1.298 0.265 0.121 0.069 0.37  0.012 0.125 0.152 0.078 0.025]\n",
      "[0.038 0.558 0.028 0.181 0.183 0.258 0.042 0.041 0.242 0.533 0.208 0.438 0.059 0.073 0.182 0.496 0.111 0.353 0.115 0.028 0.411 0.069 0.155 0.274 0.06  0.617 0.41  0.723 0.172 1.463 0.05  0.422 0.107\n",
      " 0.015 0.115 0.011 0.225 0.008 0.293 0.017 1.298 0.265 0.121 0.069 0.37  0.012 0.125 0.152 0.078 0.025]\n",
      "[0.038 0.557 0.028 0.181 0.183 0.258 0.042 0.041 0.242 0.533 0.208 0.438 0.059 0.073 0.182 0.496 0.111 0.353 0.115 0.028 0.412 0.069 0.155 0.274 0.06  0.618 0.41  0.723 0.172 1.463 0.051 0.422 0.107\n",
      " 0.015 0.115 0.011 0.225 0.008 0.293 0.017 1.298 0.265 0.121 0.069 0.37  0.012 0.125 0.152 0.078 0.025]\n",
      "[0.038 0.557 0.028 0.181 0.183 0.258 0.042 0.041 0.242 0.533 0.207 0.438 0.059 0.073 0.182 0.496 0.111 0.353 0.115 0.028 0.413 0.069 0.155 0.274 0.06  0.618 0.41  0.723 0.172 1.463 0.051 0.422 0.107\n",
      " 0.015 0.115 0.011 0.225 0.008 0.293 0.017 1.298 0.265 0.121 0.069 0.37  0.012 0.125 0.152 0.078 0.025]\n",
      "[0.038 0.557 0.028 0.182 0.183 0.258 0.042 0.041 0.242 0.533 0.207 0.438 0.059 0.073 0.182 0.496 0.111 0.353 0.115 0.028 0.413 0.069 0.155 0.275 0.06  0.618 0.41  0.723 0.172 1.463 0.052 0.422 0.107\n",
      " 0.015 0.115 0.011 0.224 0.008 0.293 0.018 1.298 0.265 0.121 0.069 0.37  0.012 0.125 0.152 0.078 0.025]\n",
      "[0.038 0.557 0.028 0.182 0.183 0.258 0.042 0.041 0.242 0.533 0.207 0.438 0.059 0.073 0.182 0.496 0.111 0.353 0.115 0.028 0.413 0.069 0.155 0.274 0.06  0.619 0.41  0.723 0.172 1.463 0.054 0.422 0.105\n",
      " 0.015 0.115 0.011 0.224 0.008 0.293 0.018 1.298 0.264 0.121 0.069 0.371 0.012 0.125 0.152 0.078 0.025]\n",
      "[0.038 0.557 0.028 0.182 0.183 0.258 0.042 0.042 0.242 0.533 0.207 0.439 0.059 0.073 0.182 0.496 0.111 0.353 0.115 0.028 0.413 0.069 0.155 0.275 0.06  0.619 0.41  0.723 0.172 1.463 0.054 0.422 0.105\n",
      " 0.015 0.115 0.011 0.224 0.008 0.293 0.018 1.298 0.264 0.121 0.069 0.371 0.012 0.125 0.151 0.078 0.025]\n",
      "[0.038 0.558 0.028 0.182 0.183 0.258 0.042 0.042 0.242 0.533 0.207 0.439 0.059 0.073 0.182 0.496 0.111 0.354 0.115 0.028 0.413 0.069 0.155 0.275 0.06  0.619 0.41  0.724 0.172 1.463 0.054 0.422 0.105\n",
      " 0.015 0.115 0.011 0.224 0.008 0.293 0.018 1.298 0.264 0.121 0.069 0.371 0.012 0.125 0.151 0.078 0.025]\n",
      "[0.038 0.558 0.029 0.181 0.183 0.258 0.042 0.042 0.242 0.533 0.207 0.439 0.059 0.073 0.182 0.496 0.111 0.354 0.114 0.028 0.414 0.069 0.155 0.275 0.06  0.619 0.41  0.724 0.172 1.463 0.055 0.422 0.105\n",
      " 0.015 0.115 0.011 0.224 0.008 0.293 0.018 1.298 0.264 0.121 0.069 0.371 0.012 0.125 0.151 0.078 0.025]\n",
      "[0.038 0.559 0.029 0.182 0.183 0.259 0.042 0.042 0.242 0.533 0.207 0.44  0.059 0.073 0.182 0.496 0.111 0.354 0.114 0.028 0.414 0.069 0.155 0.274 0.06  0.619 0.41  0.725 0.172 1.463 0.055 0.422 0.105\n",
      " 0.015 0.115 0.011 0.224 0.008 0.293 0.018 1.298 0.264 0.121 0.069 0.371 0.012 0.125 0.151 0.078 0.025]\n",
      "[0.038 0.559 0.029 0.181 0.183 0.259 0.042 0.042 0.242 0.533 0.207 0.44  0.059 0.073 0.182 0.496 0.111 0.354 0.114 0.028 0.413 0.069 0.155 0.274 0.06  0.619 0.41  0.725 0.172 1.463 0.055 0.422 0.105\n",
      " 0.015 0.115 0.011 0.224 0.008 0.293 0.018 1.298 0.264 0.121 0.069 0.371 0.012 0.125 0.151 0.078 0.025]\n",
      "[0.038 0.559 0.029 0.181 0.183 0.259 0.042 0.042 0.243 0.533 0.207 0.44  0.059 0.073 0.182 0.496 0.111 0.354 0.114 0.028 0.413 0.069 0.155 0.274 0.06  0.619 0.41  0.725 0.172 1.463 0.055 0.422 0.105\n",
      " 0.015 0.115 0.011 0.224 0.008 0.293 0.018 1.298 0.264 0.121 0.069 0.371 0.012 0.125 0.151 0.078 0.025]\n",
      "[0.038 0.559 0.029 0.181 0.183 0.259 0.042 0.042 0.243 0.533 0.207 0.44  0.06  0.073 0.182 0.496 0.111 0.354 0.114 0.028 0.413 0.069 0.155 0.274 0.06  0.619 0.41  0.724 0.172 1.463 0.056 0.422 0.105\n",
      " 0.015 0.115 0.011 0.224 0.008 0.293 0.018 1.298 0.264 0.121 0.069 0.371 0.012 0.125 0.151 0.078 0.025]\n",
      "[0.038 0.559 0.029 0.181 0.183 0.259 0.042 0.042 0.243 0.533 0.207 0.44  0.06  0.073 0.182 0.496 0.111 0.354 0.114 0.028 0.413 0.069 0.155 0.274 0.06  0.619 0.41  0.724 0.172 1.463 0.056 0.422 0.105\n",
      " 0.015 0.115 0.011 0.224 0.008 0.293 0.018 1.298 0.265 0.121 0.069 0.371 0.012 0.125 0.151 0.078 0.025]\n",
      "[0.038 0.559 0.029 0.181 0.183 0.259 0.042 0.042 0.243 0.533 0.207 0.44  0.06  0.073 0.182 0.496 0.111 0.354 0.114 0.028 0.413 0.069 0.155 0.274 0.06  0.619 0.41  0.724 0.172 1.463 0.057 0.422 0.105\n",
      " 0.015 0.115 0.011 0.224 0.008 0.293 0.019 1.298 0.265 0.121 0.069 0.371 0.012 0.125 0.151 0.078 0.025]\n",
      "[0.038 0.559 0.029 0.181 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.44  0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.413 0.069 0.155 0.274 0.06  0.619 0.41  0.724 0.172 1.463 0.057 0.422 0.105\n",
      " 0.016 0.115 0.011 0.224 0.008 0.293 0.019 1.298 0.265 0.121 0.069 0.371 0.012 0.125 0.151 0.078 0.025]\n",
      "[0.038 0.559 0.029 0.181 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.44  0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.413 0.069 0.155 0.274 0.06  0.619 0.41  0.726 0.172 1.463 0.057 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.121 0.069 0.371 0.012 0.125 0.151 0.078 0.025]\n",
      "[0.038 0.559 0.029 0.181 0.183 0.258 0.043 0.043 0.243 0.532 0.207 0.44  0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.414 0.069 0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.465 0.057 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.121 0.068 0.371 0.012 0.125 0.151 0.078 0.025]\n",
      "[0.038 0.559 0.029 0.181 0.183 0.258 0.043 0.043 0.243 0.532 0.207 0.44  0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.415 0.069 0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.465 0.057 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.121 0.068 0.371 0.012 0.125 0.151 0.079 0.025]\n",
      "[0.038 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.243 0.532 0.207 0.44  0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.415 0.069 0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.465 0.057 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.121 0.068 0.371 0.012 0.125 0.151 0.079 0.025]\n",
      "[0.039 0.559 0.029 0.181 0.183 0.258 0.043 0.043 0.243 0.532 0.207 0.44  0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.415 0.069 0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.057 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.121 0.068 0.371 0.012 0.125 0.151 0.079 0.025]\n",
      "[0.039 0.559 0.029 0.181 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.44  0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.415 0.069 0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.057 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.121 0.068 0.371 0.012 0.125 0.151 0.079 0.025]\n",
      "[0.039 0.559 0.029 0.181 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.415 0.069 0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.057 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.121 0.068 0.371 0.012 0.125 0.151 0.079 0.025]\n",
      "[0.039 0.559 0.029 0.181 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.415 0.069 0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.019 1.298 0.265 0.121 0.068 0.371 0.012 0.125 0.151 0.079 0.025]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.415 0.069 0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.121 0.068 0.371 0.012 0.125 0.151 0.079 0.025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.415 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.121 0.068 0.371 0.012 0.125 0.151 0.079 0.025]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.415 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.121 0.068 0.371 0.012 0.125 0.151 0.079 0.025]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.415 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.121 0.068 0.371 0.012 0.125 0.151 0.079 0.025]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.415 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.122 0.068 0.371 0.012 0.125 0.151 0.079 0.025]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.416 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.122 0.068 0.371 0.012 0.125 0.151 0.079 0.025]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.114 0.028 0.417 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.019 1.298 0.265 0.122 0.068 0.37  0.012 0.125 0.151 0.079 0.025]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.116 0.028 0.418 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.02  1.298 0.265 0.122 0.068 0.37  0.012 0.125 0.151 0.079 0.025]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.116 0.028 0.418 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.02  1.298 0.265 0.122 0.068 0.37  0.012 0.125 0.151 0.079 0.025]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.116 0.028 0.418 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.02  1.298 0.265 0.122 0.068 0.37  0.012 0.125 0.151 0.079 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.116 0.028 0.418 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.422 0.106\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.02  1.298 0.265 0.122 0.068 0.37  0.012 0.125 0.151 0.079 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.243 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.116 0.028 0.418 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.422 0.105\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.02  1.298 0.265 0.122 0.068 0.37  0.012 0.125 0.151 0.079 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.242 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.354 0.116 0.028 0.418 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.423 0.105\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.02  1.298 0.265 0.122 0.068 0.37  0.012 0.125 0.151 0.079 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.242 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.353 0.116 0.028 0.418 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.423 0.105\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.021 1.299 0.265 0.123 0.068 0.37  0.012 0.125 0.151 0.079 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.242 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.353 0.116 0.028 0.422 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.423 0.106\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.022 1.299 0.265 0.123 0.068 0.37  0.012 0.125 0.151 0.078 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.183 0.258 0.043 0.043 0.242 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.112 0.353 0.116 0.028 0.423 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.423 0.106\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.022 1.299 0.265 0.123 0.068 0.37  0.012 0.125 0.151 0.077 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.184 0.258 0.043 0.043 0.242 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.111 0.353 0.116 0.028 0.423 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.423 0.106\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.022 1.299 0.267 0.123 0.068 0.37  0.012 0.125 0.151 0.077 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.184 0.258 0.043 0.043 0.242 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.111 0.353 0.116 0.028 0.424 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.423 0.106\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.022 1.299 0.267 0.123 0.068 0.37  0.012 0.125 0.151 0.077 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.184 0.258 0.043 0.043 0.242 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.111 0.353 0.116 0.028 0.424 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.423 0.106\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.022 1.299 0.267 0.123 0.068 0.37  0.012 0.125 0.151 0.077 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.184 0.258 0.043 0.043 0.241 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.111 0.353 0.116 0.028 0.427 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.423 0.106\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.022 1.299 0.267 0.125 0.068 0.37  0.012 0.125 0.151 0.077 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.184 0.258 0.043 0.042 0.241 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.111 0.353 0.116 0.028 0.428 0.07  0.155 0.275 0.06  0.619 0.41  0.726 0.172 1.466 0.058 0.423 0.106\n",
      " 0.014 0.115 0.012 0.224 0.009 0.293 0.023 1.299 0.267 0.135 0.068 0.37  0.012 0.126 0.151 0.077 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.184 0.258 0.043 0.042 0.241 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.111 0.353 0.116 0.028 0.428 0.07  0.155 0.275 0.061 0.619 0.41  0.726 0.172 1.466 0.058 0.423 0.106\n",
      " 0.013 0.115 0.012 0.224 0.009 0.293 0.023 1.299 0.268 0.136 0.068 0.37  0.012 0.127 0.151 0.077 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.184 0.257 0.043 0.042 0.241 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.111 0.353 0.116 0.028 0.428 0.07  0.155 0.275 0.061 0.619 0.41  0.726 0.172 1.466 0.058 0.423 0.106\n",
      " 0.014 0.115 0.012 0.224 0.009 0.293 0.023 1.299 0.268 0.136 0.068 0.37  0.012 0.127 0.151 0.077 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.184 0.258 0.043 0.042 0.241 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.111 0.353 0.116 0.028 0.428 0.07  0.155 0.275 0.061 0.619 0.41  0.726 0.172 1.466 0.058 0.423 0.106\n",
      " 0.014 0.115 0.012 0.224 0.009 0.293 0.023 1.299 0.268 0.136 0.068 0.37  0.012 0.127 0.151 0.077 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.184 0.258 0.044 0.042 0.241 0.533 0.207 0.439 0.06  0.073 0.182 0.496 0.111 0.353 0.116 0.028 0.427 0.07  0.155 0.275 0.061 0.619 0.41  0.726 0.172 1.466 0.058 0.423 0.106\n",
      " 0.015 0.115 0.012 0.224 0.008 0.293 0.023 1.299 0.267 0.136 0.068 0.37  0.012 0.127 0.151 0.077 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.184 0.259 0.044 0.042 0.241 0.533 0.207 0.439 0.06  0.073 0.182 0.503 0.111 0.353 0.116 0.028 0.427 0.07  0.155 0.275 0.061 0.619 0.41  0.726 0.172 1.465 0.059 0.423 0.106\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.023 1.299 0.267 0.134 0.068 0.37  0.012 0.127 0.151 0.077 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.184 0.259 0.044 0.042 0.241 0.533 0.207 0.439 0.06  0.073 0.182 0.503 0.111 0.353 0.117 0.028 0.427 0.07  0.155 0.275 0.061 0.619 0.41  0.726 0.172 1.465 0.059 0.423 0.105\n",
      " 0.016 0.115 0.012 0.224 0.008 0.293 0.023 1.299 0.267 0.132 0.068 0.37  0.012 0.127 0.151 0.077 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.184 0.259 0.044 0.042 0.241 0.533 0.207 0.439 0.06  0.073 0.182 0.503 0.111 0.353 0.117 0.028 0.427 0.07  0.155 0.275 0.061 0.619 0.41  0.726 0.172 1.465 0.059 0.423 0.105\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.023 1.299 0.267 0.132 0.068 0.37  0.012 0.127 0.151 0.077 0.026]\n",
      "[0.039 0.56  0.029 0.182 0.184 0.259 0.044 0.042 0.241 0.533 0.207 0.439 0.06  0.073 0.182 0.503 0.111 0.353 0.117 0.028 0.427 0.072 0.155 0.275 0.061 0.619 0.41  0.726 0.172 1.465 0.059 0.423 0.105\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.023 1.299 0.267 0.133 0.068 0.37  0.012 0.127 0.151 0.077 0.026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.039 0.56  0.029 0.182 0.184 0.259 0.044 0.042 0.241 0.533 0.207 0.439 0.06  0.073 0.182 0.503 0.111 0.353 0.117 0.028 0.431 0.072 0.155 0.275 0.061 0.619 0.41  0.726 0.172 1.465 0.059 0.423 0.105\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.023 1.299 0.267 0.133 0.068 0.37  0.012 0.127 0.151 0.077 0.026]\n",
      "[0.039 0.559 0.029 0.182 0.184 0.259 0.044 0.042 0.241 0.533 0.207 0.44  0.06  0.074 0.182 0.503 0.111 0.353 0.117 0.028 0.433 0.073 0.155 0.275 0.061 0.619 0.41  0.726 0.172 1.465 0.059 0.423 0.105\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.023 1.299 0.267 0.133 0.068 0.37  0.012 0.127 0.151 0.076 0.026]\n",
      "[0.038 0.559 0.029 0.182 0.184 0.259 0.044 0.042 0.241 0.533 0.207 0.441 0.06  0.074 0.182 0.503 0.111 0.353 0.117 0.028 0.433 0.073 0.155 0.275 0.061 0.619 0.41  0.726 0.172 1.465 0.059 0.423 0.105\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.023 1.299 0.268 0.133 0.068 0.37  0.012 0.127 0.151 0.076 0.026]\n",
      "[0.038 0.559 0.029 0.182 0.184 0.259 0.044 0.042 0.241 0.533 0.207 0.441 0.06  0.074 0.182 0.503 0.111 0.353 0.117 0.028 0.445 0.073 0.155 0.275 0.061 0.619 0.41  0.726 0.172 1.465 0.06  0.423 0.105\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.023 1.299 0.268 0.132 0.068 0.37  0.012 0.127 0.151 0.075 0.026]\n",
      "[0.038 0.559 0.029 0.182 0.185 0.259 0.044 0.042 0.241 0.534 0.207 0.441 0.06  0.074 0.182 0.503 0.111 0.353 0.117 0.028 0.446 0.073 0.155 0.275 0.061 0.619 0.41  0.727 0.172 1.465 0.06  0.423 0.105\n",
      " 0.016 0.115 0.012 0.224 0.009 0.293 0.023 1.299 0.268 0.132 0.068 0.37  0.012 0.127 0.151 0.07  0.026]\n",
      "Overall tc: 12.396312012950297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<corextopic.corextopic.Corex at 0x1a2a13eeb8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the CorEx topic model with 50 topics\n",
    "topic_model = ct.Corex(n_hidden=50, words=words, max_iter=200, verbose=True, seed=1)\n",
    "topic_model.fit(doc_word[:len(documents)], words=words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('products', 0.15927212257342793, 1.0),\n",
       " ('industry', 0.1532496058484271, 1.0),\n",
       " ('value', 0.15260675538381344, 1.0),\n",
       " ('establishments', 0.14943021247390673, 1.0),\n",
       " ('total', 0.13742491094420908, 1.0),\n",
       " ('cent', 0.11353625541728218, 1.0),\n",
       " ('table', 0.10781344655704732, 1.0),\n",
       " ('reported', 0.10692745508750423, 1.0),\n",
       " ('statistics', 0.08780133184558349, 1.0),\n",
       " ('manufacture', 0.07388261926017761, 1.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a single topic from CorEx topic model\n",
    "topic_model.get_topics(topic=0, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: products, industry, value, establishments, total, cent, table, reported, statistics, manufacture\n",
      "1: water, wood, piece, dry, inch, surface, add, glue, cut, mold\n",
      "2: govt, repts, com, urges, pres, ct, natl, gen, conf, amer\n",
      "3: furniture, style, chairs, design, decoration, carved, designs, examples, ornament, gothic\n",
      "4: fig, ends, legs, pieces, rails, concrete, width, square, chair, placed\n",
      "5: union, workers, local, members, international, locals, organization, trade, employers, unions\n",
      "6: earners, wage, number, average, employed, prevailing, employees, nearest, representative, hours\n",
      "7: today, yesterday, american, president, national, association, committee, street, washington, mr\n",
      "8: work, time, best, quite, possible, working, present, true, matter, does\n",
      "9: bo, lesson, vegetables, girls, cooking, teacher, food, foods, cooked, minutes\n",
      "10: mills, goods, horsepower, primary, rolling, woolen, cotton, worsted, silk, purchased\n",
      "11: age, occupations, gainful, sex, years, persons, gainfully, female, occupied, divisions\n",
      "12: precinct, patrolmen, annum, assignments, varnish, duty, 18th, sulphuric, patrolman, effect\n",
      "13: york, new, pennsylvania, jersey, ohio, illinois, massachusetts, state, wisconsin, michigan\n",
      "14: market, electric, exchange, motors, generated, power, rented, current, trading, prices\n",
      "15: great, century, people, early, english, knowledge, italian, things, feel, skill\n",
      "16: institute, technology, avenue, william, mass, story, afternoon, evening, park, club\n",
      "17: north, south, carolina, west, central, virginia, east, atlantic, georgia, rhode\n",
      "18: art, exhibition, museum, artists, paintings, modern, galleries, gallery, artist, open\n",
      "19: used, iron, steel, sugar, gas, fuel, product, consumed, produced, wire\n",
      "20: law, court, war, says, supreme, action, decision, plan, london, act\n",
      "21: dr, school, university, students, schools, college, rev, education, society, teaching\n",
      "22: individual, operations, avoid, disclosing, ownership, disclosure, corporate, corporations, controlled, excluding\n",
      "23: classification, according, classified, occupational, status, introduction, comparison, earlier, laborin, compare\n",
      "24: proportion, females, tho, males, larger, constituted, smaller, greater, formed, percentage\n",
      "25: good, make, appearance, thing, spirit, fashion, way, hard, dark, room\n",
      "26: employment, minimum, maximum, labor, conditions, wages, months, gave, industrial, general\n",
      "27: mind, borne, shop, obtained, determining, worker, stated, account, purport, facts\n",
      "28: circulation, apparatus, newspapers, periodicals, electrical, publications, automobiles, motor, machinery, automobile\n",
      "29: states, united, leading, rank, statestable, variation, ranking, higher, differences, named\n",
      "30: executive, secretary, meeting, political, money, recently, human, elected, success, vice\n",
      "31: district, population, metropolitan, territory, san, francisco, outside, districts, columbia, townships\n",
      "32: fact, large, small, period, comparatively, owing, somewhat, works, difference, later\n",
      "33: cabinet, french, taste, maker, makers, france, ages, german, original, line\n",
      "34: farmers, farm, sales, agriculture, agricultural, stores, farmer, bank, retail, july\n",
      "35: demand, rates, reduced, cents, relation, currency, strict, studied, financial, disease\n",
      "36: long, old, come, set, right, need, principle, sound, servant, desire\n",
      "37: city, cities, st, places, inhabitants, philadelphia, baltimore, paul, towns, area\n",
      "38: season, dull, say, just, pretty, soon, played, close, seasons, weak\n",
      "39: income, special, absence, requirements, study, return, elementary, supervision, instruction, trained\n",
      "40: opening, point, view, final, opened, reached, goes, finally, pass, weeks\n",
      "41: men, women, day, week, month, summer, september, winter, spring, holiday\n",
      "42: given, group, ho, order, class, comprising, closely, followed, presentation, indicating\n",
      "43: book, science, books, published, written, history, publication, volume, organ, search\n",
      "44: shown, tables, page, base, slight, lower, following, showing, proportions, secured\n",
      "45: exports, rosin, imports, shoe, exported, boot, le, ga, durmg, savannah\n",
      "46: equal, plants, consumption, processes, independent, textile, custom, practically, related, rent\n",
      "47: service, domestic, rise, radio, text, research, emergency, remarks, tests, planned\n",
      "48: seen, backs, raise, theatre, failure, projects, authorities, motion, force, steps\n",
      "49: church, paris, charles, clock, series, author, love, ball, david, gray\n"
     ]
    }
   ],
   "source": [
    "for topic_n,topic in enumerate(topics):\n",
    "    # w: word, mi: mutual information, s: sign\n",
    "    topic = [(w,mi,s) if s > 0 else ('~'+w,mi,s) for w,mi,s in topic]\n",
    "    # Unpack the info about the topic\n",
    "    topic_words,mis,signs = zip(*topic)\n",
    "    # Print topic\n",
    "    topic_str = str(topic_n)+': '+', '.join(topic_words)\n",
    "    print(topic_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = topic_model.predict(doc_word[len(documents):])\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_result(results):\n",
    "    pairs=[]\n",
    "    for i in range(len(results)):\n",
    "        for j in range(len(results[i])):\n",
    "            if results[i][j]==True:\n",
    "                pairs.append([i,j])\n",
    "    return pairs\n",
    "\n",
    "def count_topics(pairs):\n",
    "    available={}\n",
    "    for a,b in pairs:\n",
    "        if b not in available.keys():\n",
    "            available[b]=1\n",
    "        else:\n",
    "            available[b]+=1\n",
    "    return available \n",
    "\n",
    "def get_topic_content(pairs,topic):\n",
    "    result=[]\n",
    "    for a,b in pairs:\n",
    "        if b == topic:\n",
    "            result.append(occ_list[a])\n",
    "    return result \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Corelation and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.396312012950295"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.tc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.tcs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.396312012950295\n",
      "12.396312012950295\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(topic_model.tcs))\n",
    "print(topic_model.tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting number of topics:Choosing from the data visualised\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_model.tcs.shape[0]), topic_model.tcs, color='#4e79a7', width=0.5)\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16);\n",
    "plt.savefig('Distribution of TCs for each topic', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Document TC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14555, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.log_z.shape # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.465 1.299 0.727 0.619 0.559 0.534 0.503 0.446 0.441 0.423 0.41  0.37  0.353 0.293 0.275 0.268 0.259 0.241 0.224 0.207 0.185 0.182 0.182 0.172 0.155 0.151 0.132 0.127 0.117 0.115 0.111 0.105 0.074\n",
      " 0.073 0.07  0.068 0.061 0.06  0.06  0.044 0.042 0.038 0.029 0.028 0.026 0.023 0.016 0.012 0.012 0.009]\n",
      "[1.465 1.299 0.727 0.619 0.559 0.534 0.503 0.446 0.441 0.423 0.41  0.37  0.353 0.293 0.275 0.268 0.259 0.241 0.224 0.207 0.185 0.182 0.182 0.172 0.155 0.151 0.132 0.127 0.117 0.115 0.111 0.105 0.074\n",
      " 0.073 0.07  0.068 0.061 0.06  0.06  0.044 0.042 0.038 0.029 0.028 0.026 0.023 0.016 0.012 0.012 0.009]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(topic_model.log_z, axis=0)) #The pointwise total correlations in log_z represent the correlations within an individual document explained by a particular topic. These correlations have been used to measure how \"surprising\" documents are with respect to given topics\n",
    "print(topic_model.tcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Anchoring in the semi-supervised topic mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CorEx is a discriminative model, whereas LDA is a generative model. This means that while LDA outputs a probability distribution over each document, CorEx instead estimates the probability a document belongs to a topic given that document's words. As a result, the probabilities across topics for a given document do not have to add up to 1. The estimated probabilities of topics for each document can be accessed through log_p_y_given_x or p_y_given_x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Topic Models\n",
    "The labels attribute gives the binary topic expressions for each document and each topic. We can use this output as input to another CorEx topic model to get latent representations of the topics themselves. This yields a hierarchical CorEx topic model. Like the first layer of the topic model, one can determine the number of latent variables to add in higher layers through examination of the topic TCs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anchored CorEx is an extension of CorEx that allows the \"anchoring\" of words to topics. When anchoring a word to a topic, CorEx is trying to maximize the mutual information between that word and the anchored topic. So, anchoring provides a way to guide the topic model towards specific subsets of words that the user would like to explore.\n",
    "\n",
    "1. Anchoring a single set of words to a single topic. This can help promote a topic that did not naturally emerge when running an unsupervised instance of the CorEx topic model. For example, one might anchor words like \"snow,\" \"cold,\" and \"avalanche\" to a topic if one suspects there should be a snow avalanche topic within a set of disaster relief articles.\n",
    "\n",
    "2. Anchoring single sets of words to multiple topics. This can help find different aspects of a topic that may be discussed in several different contexts. For example, one might anchor \"protest\" to three topics and \"riot\" to three other topics to understand different framings that arise from tweets about political protests.\n",
    "\n",
    "3. Anchoring different sets of words to multiple topics. This can help enforce topic separability if there appear to be chimera topics. For example, one might anchor \"mountain,\" \"Bernese,\" and \"dog\" to one topic and \"mountain,\" \"rocky,\" and \"colorado\" to another topic to help separate topics that merge discussion of Bernese Mountain Dogs and the Rocky Mountains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['products', 'industry', 'value'],\n",
       " ['water', 'wood', 'piece'],\n",
       " ['govt', 'repts', 'com'],\n",
       " ['furniture', 'style', 'chairs'],\n",
       " ['fig', 'ends', 'legs'],\n",
       " ['union', 'workers', 'local'],\n",
       " ['earners', 'wage', 'number'],\n",
       " ['today', 'yesterday', 'american'],\n",
       " ['work', 'time', 'best'],\n",
       " ['bo', 'lesson', 'vegetables'],\n",
       " ['mills', 'goods', 'horsepower'],\n",
       " ['age', 'occupations', 'gainful'],\n",
       " ['precinct', 'patrolmen', 'annum'],\n",
       " ['york', 'new', 'pennsylvania'],\n",
       " ['market', 'electric', 'exchange'],\n",
       " ['great', 'century', 'people'],\n",
       " ['institute', 'technology', 'avenue'],\n",
       " ['north', 'south', 'carolina'],\n",
       " ['art', 'exhibition', 'museum'],\n",
       " ['used', 'iron', 'steel'],\n",
       " ['law', 'court', 'war'],\n",
       " ['dr', 'school', 'university'],\n",
       " ['individual', 'operations', 'avoid'],\n",
       " ['classification', 'according', 'classified'],\n",
       " ['proportion', 'females', 'tho'],\n",
       " ['good', 'make', 'appearance'],\n",
       " ['employment', 'minimum', 'maximum'],\n",
       " ['mind', 'borne', 'shop'],\n",
       " ['circulation', 'apparatus', 'newspapers'],\n",
       " ['states', 'united', 'leading'],\n",
       " ['executive', 'secretary', 'meeting'],\n",
       " ['district', 'population', 'metropolitan'],\n",
       " ['fact', 'large', 'small'],\n",
       " ['cabinet', 'french', 'taste'],\n",
       " ['farmers', 'farm', 'sales'],\n",
       " ['demand', 'rates', 'reduced'],\n",
       " ['long', 'old', 'come'],\n",
       " ['city', 'cities', 'st'],\n",
       " ['season', 'dull', 'say'],\n",
       " ['income', 'special', 'absence'],\n",
       " ['opening', 'point', 'view'],\n",
       " ['men', 'women', 'day'],\n",
       " ['given', 'group', 'ho'],\n",
       " ['book', 'science', 'books'],\n",
       " ['shown', 'tables', 'page'],\n",
       " ['exports', 'rosin', 'imports'],\n",
       " ['equal', 'plants', 'consumption'],\n",
       " ['service', 'domestic', 'rise'],\n",
       " ['seen', 'backs', 'raise'],\n",
       " ['church', 'paris', 'charles']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to automatically generate anchor words: for each label in a data set, \n",
    "# we find the words that have the highest mutual information with the label.\n",
    "# we took a very simple to automatically generate the anchor words to create a semi-supervised model\n",
    "\n",
    "anchor_words=[]\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_,_ = zip(*topic)\n",
    "    anchor_words.append(list(topic_words[:3]))\n",
    "\n",
    "anchor_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anchor_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Some words never appear (or always appear)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9222"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anchor 'nasa' and 'space' to first topic, 'sports' and 'stadium' to second topic, so on...\n",
    "#anchor_words = [['industry', 'manufacture','worker'], ['professional','skilled','technology'], ['politics', 'government'], ['domestic','service']]\n",
    "\n",
    "anchored_topic_model = ct.Corex(n_hidden=50, seed=2) # note different seed, does it matter?\n",
    "anchored_topic_model.fit(doc_word[:len(documents)], words=words, anchors=anchor_words, anchor_strength=6) # anchor_strength pretty high\n",
    "\n",
    "doc_word.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: industry,products,value,manufacture,materials,added,branches,primarily,valued,duplication\n",
      "1: wood,water,piece,dry,add,boil,brush,stain,surface,hot\n",
      "2: govt,com,repts,pres,urges,natl,conf,ussr,ct,gen\n",
      "3: furniture,style,chairs,design,decoration,carved,designs,examples,ivory,gothic\n",
      "4: fig,ends,legs,inch,mold,sides,cut,inches,pieces,cast\n",
      "5: workers,union,local,members,international,locals,trade,shops,employers,strike\n",
      "6: number,wage,earners,employed,average,employees,prevailing,gives,hours,december\n",
      "7: today,yesterday,american,announced,ap,afternoon,died,aug,company,kindred\n",
      "8: work,time,best,doing,performed,overtime,paid,complete,begin,carvers\n",
      "9: bo,lesson,vegetables,cooking,girls,teacher,food,milk,foods,cooked\n",
      "10: mills,goods,horsepower,primary,cotton,rolling,worsted,woolen,silk,purchased\n",
      "11: age,occupations,gainful,sex,engaged,years,persons,occupational,gainfully,status\n",
      "12: precinct,annum,patrolmen,assignments,duty,division,appointed,18th,patrolman,effect\n",
      "13: new,york,pennsylvania,jersey,ohio,massachusetts,illinois,editor,wisconsin,times\n",
      "14: market,electric,exchange,motors,generated,rented,power,current,trading,prices\n",
      "15: great,people,century,sixteenth,fifteenth,eighteenth,twelfth,britain,majority,later\n",
      "16: institute,technology,avenue,street,story,team,building,film,mass,game\n",
      "17: south,north,carolina,central,east,west,atlantic,georgia,virginia,mississippi\n",
      "18: art,exhibition,museum,artists,paintings,galleries,gallery,modern,open,artist\n",
      "19: used,steel,iron,fuel,works,kinds,pig,furnaces,coal,blast\n",
      "20: war,law,court,supreme,justice,act,judge,courts,judges,martial\n",
      "21: school,dr,university,students,education,schools,universities,examination,examinations,college\n",
      "22: individual,operations,avoid,disclosing,disclosure,excluding,purport,depreciation,profits,expensesas\n",
      "23: according,classification,classified,laborin,grouped,worked,establishmentstable,arranged,making,classifies\n",
      "24: tho,proportion,females,males,larger,cont,varied,smaller,aro,somo\n",
      "25: good,make,appearance,possible,grade,fairly,help,eighth,simpler,pleasing\n",
      "26: employment,minimum,maximum,gave,month,boldface,italic,months,january,seasonal\n",
      "27: shop,mind,borne,estimate,described,industrytable,chairmen,foreman,considering,sweat\n",
      "28: total,establishments,cent,reported,table,statistics,census,newspapers,circulation,increase\n",
      "29: states,united,leading,state,ranked,statestable,rank,michigan,california,indiana\n",
      "30: meeting,executive,secretary,president,committee,convention,board,organization,national,labor\n",
      "31: district,population,metropolitan,territory,columbia,outside,districts,area,townships,places\n",
      "32: large,small,fact,comparatively,quantities,wares,owing,difference,establishmentsthat,distinguish\n",
      "33: french,cabinet,taste,maker,makers,designer,fashion,mahogany,german,english\n",
      "34: farmers,farm,sales,stores,reserve,agriculture,retail,department,federal,farmer\n",
      "35: demand,reduced,rates,posted,cable,sterling,bills,actual,commercial,foreign\n",
      "36: long,old,come,ago,standing,pulls,egyptians,acquire,saying,substituted\n",
      "37: city,cities,st,inhabitants,paul,towns,philadelphia,baltimore,cincinnati,township\n",
      "38: season,say,dull,man,life,things,know,present,way,thing\n",
      "39: special,income,absence,health,schedule,tuberculosis,certain,physical,occupation,relation\n",
      "40: point,view,opening,convenience,reached,reach,forward,basing,intersection,shoulders\n",
      "41: day,men,women,nearest,representative,15th,monthstable,hour,weak,franchise\n",
      "42: given,group,ho,column,directions,specific,boon,printed,arc,consolidated\n",
      "43: book,science,books,advancement,published,search,captions,writers,metadata,transcripts\n",
      "44: shown,tables,page,secured,following,distinction,rent,gristmills,viii,omissions\n",
      "45: exports,rosin,imports,turpentine,oil,till,clean,process,dust,sand\n",
      "46: equal,plants,consumption,packing,slaughtering,meat,sugar,beet,cane,molasses\n",
      "47: service,domestic,rise,personal,bureau,professional,contents,civil,jobs,volunteer\n",
      "48: seen,backs,raise,shrines,degradation,role,chest,exceedingly,effected,iy\n",
      "49: church,charles,paris,rev,mr,london,france,death,bishop,clerical\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.25746662529172\n",
      "55.25746662529172\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(anchored_topic_model.tcs))\n",
    "print(anchored_topic_model.tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14555, 13025)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents), 13025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14991, 9222)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1966, 50)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=topic_model.predict(doc_word[13025:])\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{7: 899, 15: 151, 25: 104, 36: 115, 2: 361, 20: 429, 48: 172, 21: 318, 30: 273, 16: 319, 41: 193, 47: 198, 40: 154, 33: 60, 43: 117, 29: 60, 49: 207, 26: 108, 37: 77, 44: 39, 34: 355, 8: 147, 39: 105, 46: 74, 35: 35, 38: 117, 13: 127, 18: 198, 5: 101, 3: 33, 32: 46, 24: 8, 42: 24, 27: 34, 17: 37, 45: 20, 1: 3, 4: 1, 9: 14, 0: 18, 22: 3, 28: 30, 31: 9, 19: 38, 10: 32, 14: 24, 6: 1, 11: 62, 23: 3, 12: 4}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3e8338fe2266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_predict_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_topic_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-e0d4b5f5a771>\u001b[0m in \u001b[0;36mget_topic_content\u001b[0;34m(pairs, topic)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mocc_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pairs=get_predict_result(results)\n",
    "print(count_topics(pairs))\n",
    "print(get_topic_content(pairs,17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_list(pairs):\n",
    "    result={}\n",
    "    for a,b in pairs:\n",
    "        if b not in result.keys():\n",
    "            result[b]=[a]\n",
    "        else:\n",
    "            result[b].append(a)\n",
    "    return result\n",
    "\n",
    "def save_topic(pairs):\n",
    "    result_dic=get_topic_list(pairs)\n",
    "    with open(\"train_result.txt\",'w') as f:\n",
    "        for m,n in result_dic.items():\n",
    "            topic_words,_,_ = zip(*anchored_topic_model.get_topics(topic=m))\n",
    "            title=str(m)+\":\"+(','.join(topic_words))\n",
    "            f.write(title+'\\n')\n",
    "            for file in n:\n",
    "                occ_title=occ_list[file]\n",
    "                f.write(occ_title+'\\n')\n",
    "            f.write(\"\\n \\n \\n\")\n",
    "\n",
    "save_topic(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
